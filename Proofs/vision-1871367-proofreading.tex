%  LaTeX support: latex@mdpi.com 
%  For support, please attach all files needed for compiling as well as the log file, and specify your operating system, LaTeX version, and LaTeX editor.

%=================================================================
\documentclass[vision,article,accept,pdftex,moreauthors]{Definitions/mdpi} 
% For posting an early version of this manuscript as a preprint, you may use "preprints" as the journal and change "submit" to "accept". The document class line would be, e.g., \documentclass[preprints,article,accept,moreauthors,pdftex]{mdpi}. This is especially recommended for submission to arXiv, where line numbers should be removed before posting. For preprints.org, the editorial staff will make this change immediately prior to posting.

%----------
% submit
%----------
% The class option "submit" will be changed to "accept" by the Editorial Office when the paper is accepted. This will only make changes to the frontpage (e.g., the logo of the journal will get visible), the headings, and the copyright information. Also, line numbering will be removed. Journal info and pagination for accepted papers will also be assigned by the Editorial Office.


%=================================================================
% MDPI internal commands
\firstpage{1} 
\makeatletter 
\setcounter{page}{\@firstpage} 
\makeatother
\pubvolume{1}
\issuenum{1}
\articlenumber{0}
\pubyear{2022}
\copyrightyear{2022}
\externaleditor{Academic Editor:} 


\datereceived{01 August 2022} 
\dateaccepted{02 November 2022} 
\datepublished{} 
%\datecorrected{} % Corrected papers include a "Corrected: XXX" date in the original paper.
%\dateretracted{} % Corrected papers include a "Retracted: XXX" date in the original paper.
\hreflink{https://doi.org/} % If needed use \linebreak
%\doinum{}
%------------------------------------------------------------------
% The following line should be uncommented if the LaTeX file is uploaded to arXiv.org
%\pdfoutput=1

%=================================================================
% Add packages and commands here. The following packages are loaded in our class file: fontenc, inputenc, calc, indentfirst, fancyhdr, graphicx, epstopdf, lastpage, ifthen, lineno, float, amsmath, setspace, enumitem, mathpazo, booktabs, titlesec, etoolbox, tabto, xcolor, soul, multirow, microtype, tikz, totcount, changepage, attrib, upgreek, cleveref, amsthm, hyphenat, natbib, hyperref, footmisc, url, geometry, newfloat, caption

%=================================================================
%% Please use the following mathematics environments: Theorem, Lemma, Corollary, Proposition, Characterization, Property, Problem, Example, ExamplesandDefinitions, Hypothesis, Remark, Definition, Notation, Assumption
%% For proofs, please use the proof environment (the amsthm package is loaded by the MDPI class).

%=================================================================
% Full title of the paper (Capitalized)
\Title{A Bayesian Statistical Model Is Able to Predict Target-by-Target Selection Behaviour in a Human Foraging Task}

% MDPI internal command: Title for citation in the left column
\TitleCitation{A Bayesian Statistical Model Is Able to Predict Target-by-Target Selection Behaviour in a Human Foraging Task}

% Author Orchid ID: enter ID or remove command
\newcommand{\orcidauthorA}{0000-0002-7368-2351} % Add \orcidA{} behind the author's name
\newcommand{\orcidauthorB}{0000-0002-4122-9499} % Add \orcidB{} behind the author's name
\newcommand{\orcidauthorC}{0000-0003-2677-1965}

% Authors, for the paper (add full first names)
\Author{\hl{Alasdair D.F. Clarke} $^{1,}$\hl{*}\orcidA{}, Amelia R. Hunt $^{2}$\orcidB{} and Anna E. Hughes~\orcidC{} $^{1}$}
%MDPI: Please carefully check the accuracy of names and affiliations. Please make sure the Address information is sorted from subordinate to superior: Center / Laboratory / Group / Programme / Unit < Department / Division / Faculty / Campus / Institute / School < College / University

%MDPI: According to our system, the correct correspondence author is Alasdair Clarke. Please confirm. RESPONSE: Yes, confirmed.
%MDPI:This is an important note to let you know that to protect the privacy of the author's contact information, we will only display the corresponding authors' contact information on the published paper. However, we hope the contact information of the other authors can also be confirmed or corrected during the proofreading stage, which will be recorded in our database to furnish the author's integrated publication history and also contribute to our future communications. RESPONSE: Okay, all confirmed.


% MDPI internal command: Authors, for metadata in PDF
\AuthorNames{Alasdair D.F. Clarke, Amelia R. Hunt and Anna E. Hughes}

% MDPI internal command: Authors, for citation in the left column
\AuthorCitation{Clarke, A.D.F.; Hunt, A.R.; Hughes, A.E.}
% If this is a Chicago style journal: Lastname, Firstname, Firstname Lastname, and Firstname Lastname.

% Affiliations / Addresses (Add [1] after \address if there is only one affiliation.)
\address{%
$^{1}$ \quad Department of Psychology, University of Essex, Wivenhoe Park, Colchester~CO4 3SQ,~UK; \hl{anna.hughes@essex.ac.uk} \\
%MDPI: added the emails from the submitting system, please confirm. RESPONSE: Confirmed.

$^{2}$ \quad School of Psychology, University of Aberdeen, King's College, Aberdeen~AB24 3FX,~UK; \hl{a.hunt@abdn.ac.uk} }

% Contact information of the corresponding author
\corres{Correspondence: a.clarke@essex.ac.uk}

% Current address and/or shared authorship
%\firstnote{Current address: Affiliation 3} 
%\secondnote{These authors contributed equally to this work.}
% The commands \thirdnote{} till \eighthnote{} are available for further notes

%\simplesumm{} % Simple summary

%\conference{} % An extended version of a conference paper

% Abstract (Do not insert blank lines, i.e., \\) 
\abstract{Foraging refers to search involving multiple targets or multiple types of targets, and as a model task has a long history in animal behaviour and human cognition research. Foraging behaviour is usually operationalized using summary statistics, such as average distance covered during target collection (the path length) and the frequency of switching between target types. We recently introduced an alternative approach, which is to model each instance of target selection as random selection without replacement. Our model produces estimates of a set of foraging biases, such as a bias to select closer targets or targets of a particular category. Here we apply this model to predict individual target selection events. We add a new start position bias to the model, and generate foraging paths using the parameters estimated from individual participants’ pre-existing data. The model predicts which target the participant will select next with a range of accuracy from 43\% to 69\% across participants (chance is 11\%). The model therefore explains a substantial proportion of foraging behaviour in this paradigm. The situations where the model makes errors reveal useful information to guide future research on those aspects of foraging that we have not yet explained.}

% Keywords
\keyword{foraging; visual search; bayesian model; decision; strategy} 

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% The order of the section titles is: Introduction, Materials and Methods, Results, Discussion, Conclusions for these journals: aerospace,algorithms,antibodies,antioxidants,atmosphere,axioms,biomedicines,carbon,crystals,designs,diagnostics,environments,fermentation,fluids,forests,fractalfract,informatics,information,inventions,jfmk,jrfm,lubricants,neonatalscreening,neuroglia,particles,pharmaceutics,polymers,processes,technologies,viruses,vision


\section{Introduction}

\hl{Foraging}, the act of searching for and gathering multiple targets (such as food), has been studied in both nonhuman animal contexts \cite{dawkins1971} and in human psychology studies~\cite{kristjansson2014}. Foraging engages a wide range of different perceptual, cognitive, decision-related and motor skills, and is an ecologically relevant behaviour for most species, including humans. For these and other reasons, a sustained interest in foraging has developed our understanding of how we search for multiple instances of multiple types of targets. For example, the classic marginal value theorem \cite {charnov1976} is generally good at predicting when an organism will decide to stop searching in a patch and move on to another, based on the finding rate dropping below an expected rate, and taking into account the relative energy costs of traveling between patches versus staying within a patch. There has also been a concerted effort to characterize the spatial patterns of foraging behaviour. The Lévy Walk, for example, has been argued to be a good description of the foraging path many species take when the locations of targets are unknown (e.g., \citep {bartumeus2005}, but see also \citep{benhamou2007}). %MDPI:Please make sure that all genes and species names as well as all the variables like "p" or "n" are written in italic font. Protein names, Latin words and genera/families should be kept in a normal font. Greek/Latin expressions and non-common foreign words and phrases should NOT be in Italic format.
%MDPI:  Abbreviations need to be defined, on their first mention, once in the Abstract, once in the Main Text, once in Figures' description, and once in the Tables' footer. Abbreviations used as keywords should be defined too.Please check and make sure that this rule is applied through the whole paper.
% RESPONSE: Checked.


Another general principle of foraging is that of the “search image”, which describes the perceptual features that a foraging animal can use to identify targets or classes of targets \cite{Dukas1993}. A limited capacity for complex search images means that dividing attention over more than one kind of potential search target can impede search, particularly when what distinguished targets from distractors is not a simple feature like colour or motion. As a result, foragers tend to search in “runs” of one type of target before switching to another. In humans, this behaviour has been studied using computer-based displays of targets and distractors (e.g., \citep{kristjansson2014}), in which participants must “collect” targets (by clicking or tapping on them) and ignore distractors. When targets can be identified based on a single, easily distinguishable feature (for example, find all the red and green shapes and ignore the blue and yellow shapes) participants tend to switch frequently between different types of targets, taking a relatively efficient path through the array. In contrast, when targets are defined based on a combination of more than one feature (e.g., find all the red squares and green circles and ignore the red circles and green squares), participants tend to switch far less frequently, often selecting all the targets of one category and then all the targets of the other. Consequently, the path taken to collect the targets is less efficient. In other words, frequent switching between target types limits the distance travelled between targets, while collecting all the targets of each type before switching to the other type sacrifices some efficiency of movement in service of reducing the mental workload. 

Different aspects of foraging behaviour, such as the search image and the search path, clearly interact with one another, but most research has tried to understand them separately. In many cases, the pattern of foraging behaviour has been studied using aggregate measures calculated on the level of a trial, such as the mean number of 'runs' (where the same type of target is selected multiple times in a row) or the total number of targets found during the longest run. These summary statistics can provide a broad view of switching behaviour, but they can also be biased by the spatial distribution of target types, and by a forager's preference for one target or another. To address the need for a more precise set of measurements, we recently developed a new model to analyse foraging data, based on a sampling without replacement procedure \cite{clarke2022foraging}. By using this model, we demonstrated that we were able to break down foraging into a number of different cognitive biases, such as a preference to stick to the same target type, or a preference for nearby targets, and used this model to successfully reanalyse data from a number of open access datasets \cite{kristjansson2014, thornton2022foraging, clarke2022, tagu_kristjansson_2021}. 

While our model is able to do a good job of allowing us to understand biases at the level of a trial (e.g., conjunction vs. feature search differences \cite{kristjansson2014} or the influence of high value targets compared to low value targets \cite{tagu2020}), we did not originally investigate to what extent our model was able to predict target-by-target behaviour within any given trial. However, as the model is based on target-by-target level information, it is possible for us to use this information to ask: to what extent does our model predict foraging behaviour within a trial? Can it predict exactly which target a participant will pick next? If the model makes mistakes, can we understand where it is failing, in order to improve our understanding of how human foraging behaviour operates?

The original implementation of our model has little to say about the initial target selection in each trial: as there is not yet a previous target, the stick/switch and proximity parameters are all ignored leaving just a simple salience parameter that allows us to model one set of targets as being more attractive than the other. Taking inspiration from the work on the central viewing bias in the eye movements literature \cite{clarke2014deriving, clarke2017} we develop two models of the spatial bias in initial target selection during visual foraging.  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Materials and Methods}

\subsection{Datasets}

We use a previously published dataset to explore how well our model can account for trial-level behaviour: the visual foraging data from Clarke and colleagues \cite{clarke2022}. This is an attractive dataset for our needs as it is very close to the ``classic'' visual foraging paradigm conducted by Kristjánsson and colleagues \citep{kristjansson2014}, yet with a substantially larger sample of participants. (The key difference between paradigms is that \cite{kristjansson2014} used an ipad and finger foraging, while \cite{clarke2022} used a desktop computer and mouse clicks.) Furthermore, we have already demonstrated that our model can capture individual differences in foraging behaviour \citep{clarke2022}. As such, we will only give a brief overview of the data here and refer the reader to these earlier papers for more details.


\subsection{A Model for Visual Foraging}

We will make use of the foraging model from Clarke et al. (2022) \cite{clarke2022foraging}. This treats foraging as a sampling without replacement process in which each item $i$ has probability $p_i$ of being selected as the next target. The $p_i$ depend on four parameters:

\begin{itemize}
    \item $b_A$---preference for selecting items of type $A$ rather than $B$.
    \item $b_S$---preference for selecting items of the same type as the previously selected item.
    \item $\sigma_d$---preference for selecting items close to the previously selected item.
    \item $\sigma_{\theta}$---preference to keep selecting items along a straight line versus changing direction.
\end{itemize}

These combine to give:
\begin{linenomath}
\begin{equation}
    w_i = g\left(b_at_i + b_sm(t_i, t_{i-1})\right) \times \rho_d(i, i-1) \rho_{\theta}(i, i-1)
\end{equation}
\end{linenomath}
where $t_i = 1$ if item $i$ is of class $A$ and 0 otherwise, and  $m(t_i, t_{i-1}) =1$ if item $i$ is the same class as the previously selected item. $\rho_d$ and $\rho_{\theta}$ measure proximity and the effect of~direction:
\begin{linenomath}
\begin{equation}
    \rho_d = e^{-\sigma_dd(i,j)}
\end{equation}
\end{linenomath}
\begin{linenomath}
\begin{equation}
    \rho_{\theta} = e^{-\sigma_d\theta(i,j)}
\end{equation}
\end{linenomath}
$d(i,j)$ is simply the Euclidean distance between items $i$ and $j$, while $\theta$ is defined as:
\begin{linenomath}
\begin{equation}
    \theta(i,j) = \frac{f(\text{atan2}(i, j) - \text{atan2}(i-1, i))}{\pi}
\end{equation}
\end{linenomath}
with $f(\phi_1, \phi_2) = \text{min}((\phi_1 - \phi_2) \% 2\pi, (\phi_2 - \phi_1) \% 2\pi)$ calculating the angular difference. $\text{atan2}$ is the direction of travel from \textit{i} to \textit{j}.  This model is implemented in a multilevel framework, allowing each of the four parameters to vary from participant to participant. Further details including priors and full code can be found with \cite{clarke2022foraging}.

Note: while our model returns estimates of full posterior probability distribution for each parameter, to reduce computational complexity and make it easier to compare to the empirical data, we will work with the means of these distributions for our parameter~values. 

\subsection{Software Environment}
 
We mainly \hl{used} R v4.2.0 and rStan v2.26.11 \cite{rstan} (R Foundation for Statistical Computing, Vienna, Austria). For fitting the model to the data from~\cite{clarke2022} we used a university computing cluster with R v3.6.3 and rStan v2.19.2.%MDPI:For all equipment/software/products used, the following information should be given: company name, city, state abbreviation (if its from USA or Canada), and country must be provided. Please apply this rule through the whole Materials and Methods section RESPONSE: Done.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results: Evaluating the Model}

To assess how well our model can account for behaviour at the level of individual trials, we start by stepping through each trial in the data from Clarke and colleagues \cite{clarke2022}. For each target selection \hl{(}Note: for the reasons discussed above, we ignore the initial target selection for now, as the model has very little to say about it. We return to this issue in Section \ref{s4}\hl{)}, %MDPI: footer is not allowed in this type article, we moved it to the (), please confirm. RESPONSE: Confirmed
in each trial, we can estimate how likely each of the remaining items are to be selected using the parameters from our posterior model. As can be seen in Figure \ref{fig:cal}, the model is putting somewhere between 25\% and 75\% of the weight on the target that is selected next, easily outperforming a chance $1/n$ baseline. There are also interesting differences between the two conditions. For example, in the conjunction condition, there is a clear 'jump' around target 20, which probably reflects the tendency for participants to forage in runs, selecting all exemplars of one target and then all exemplars of the other: there is a decrease in accuracy as people switch and then an increase again. We can also see that the model is well calibrated in that the probabilities assigned to the most likely target manage to capture how often that target is actually selected. However, we can see a decrease in accuracy when it comes to how often ``runner-up`` candidate items are selected as our model appears to systematically undervalue these (presumably by putting too much of the probabilistic weight on low chance items).

\begin{figure}[H]
\begin{adjustwidth}{-\extralength}{0cm}
\centering
\includegraphics[width=17.8 cm]{Figures/qjep_preds.pdf}
\end{adjustwidth}
\caption{({\textbf{\hl{a}}}) Top left: Posterior probabilities for target selections during a visual foraging task. Each dot shows the data from an individual participant, averaged over trials, in the feature condition and the shaded region indicates the interval in which we expect 67\% of participants to fall. The dashed line indicates chance performance. Top right: As top left, except showing the conjunction condition. (\textbf{\hl{b}})  Calibration plot for our foraging model. The $x$-axis gives the largest weight assigned by the model while the $y$-axis shows how often that target was actually selected by a human participant. The dashed line here and in the right plot is the identity line. (\textbf{\hl{c}}) This plot shows how often the model selects the 2nd and 3rd ranked items based on the weights assigned by the model.} %mdpi: we removed the subfigure label from italic to be not italic and added bold of it according to our rule, please confirm, same in below figures. RESPONSE: Confirmed.

% Also, please divide the figure into 4 subfigures and  note them accordingly. Subfigures should be numbered by Latin letters with parentheses, e.g., (a), (b), (c), or (A), (B), (C), etc., which should be put below the image or within the image; only Latin letters should be bold; parentheses should be whole. Subfigures should be provided with sub-caption and put sub-caption in figure caption. RESPONSE: Done.


%MDPI: Please confirm that all the figures are the original work of this manuscript’s authors.  If they have been adapted or they belong to another author/publisher, it’s required to send us the copyright agreement for using it. Thank you for your cooperation! RESPONSE: We confirm that all the figures are original.

\label{fig:cal}
\end{figure}   

We summarise accuracy for each person by calculating the mean proportion of times the model assigns the most weight to the item that was then selected for each participant \hl{(}This metric isn't perfect as it averages over all target selections within a trial, and these clearly have different baseline probabilities: 2.5--100\%. However, it has the upside of being more intuitive than anything else we could think of\hl{)}. %MDPI: footer is not allowed in this type article, we moved it to the (), please confirm. RESPONSE: Slightly modified, but confirmed.
These accuracy scores can be compared to our uniform chance baseline that gives a value of $10.9\%$ over the course of a trial. Figure \ref{fig:cal2} hints at some individual differences when it comes to how predictable the model is: we can see a lot of variation in the size of the weights. However, interestingly, we find that this variability can be explained by our model parameters: nearly all of the differences between different participants, as well as the within-participant differences in the feature and conjunction conditions, can be explained by the $bP$ parameter, which is our proximity bias. The model does a better job of predicting which will be selected next when there is a stronger proximity bias (see {\hl{Supplementary Materials: Part 1}} for more details).



We now look at some example trials and compare the behaviour of our human participants to the predicted behaviour from our model (see Figure \ref{fig:qjep_paths}). When looking at trials in which our model has done a particularly good job of accounting for the target selections (model accuracy > 80$\%$) we can see that the \textit{disagreements} often occur in cases when the human participant appears to be carrying out some form of local path length optimisation. In general, the cases in which our model suggests a different target from the one the participant actually selected appear reasonable. 


\begin{figure}[H]
\includegraphics[width=13.5 cm]{Figures/qjep_indiv_diff.pdf}
\caption{({\textbf{\hl{a}}}) Prediction scores for participants. Boxplots show quartile range and the grey lines indicate individual participants. The dots indicate outliers. (\textbf{\hl{b}}) How accuracy of our model varies with the strength of an individual’s \textit{bM}, \textit{bP}, \textit{pA} and \textit{pS} parameters. We can see two clear outlier participants (marked with an X).} %MDPI: please divide the figure into 2 subfigures and  note them accordingly. Subfigures should be numbered by Latin letters with parentheses, e.g., (a), (b), (c), or (A), (B), (C), etc., which should be put below the image or within the image; only Latin letters should be bold; parentheses should be whole. Subfigures should be provided with sub-caption and put sub-caption in figure caption. RESPONSE: Done.
%MDPI: Please confirm that all the figures are the original work of this manuscript’s authors. If they have been adapted or they belong to another author/publisher, it’s required to send us the copyright agreement for using it.Thank you for your cooperation! RESPONSE: Confirmed.

\label{fig:cal2}
\end{figure}   
\vspace{-12pt}



\begin{figure}[H]
\includegraphics[width=12 cm]{Figures/qjep_ex_paths.pdf}
\caption{(\textbf{\hl{a}}) and (\textbf{\hl{b}}) Two randomly selected trials in which the model does a good job in accounting for human behaviour. The numbers indicate the order the participant selected targets in. Red arrows indicate places where the model prediction deviates from participant behaviour. When participants diverge from the model's prediction, it appears to be due to some form of path-length optimisation. (\textbf{\hl{c}}) and (\textbf{\hl{d}}) Two randomly selected examples in which the model does a less good job in accounting for the order in which human participants selected the items.}%MDPI: please divide the figure into subfigures and  note them accordingly. Subfigures should be numbered by Latin letters with parentheses, e.g., (a), (b), (c), or (A), (B), (C), etc., which should be put below the image or within the image; only Latin letters should be bold; parentheses should be whole. Subfigures should be provided with sub-caption and put sub-caption in figure caption. Please add explanation for all the symbols/colors RESPONSE: Done
%MDPI: Please confirm that all the figures are the original work of this manuscript’s authors. If they have been adapted or they belong to another author/publisher, it’s required to send us the copyright agreement for using it. Thank you for your cooperation! RESPONSE: Confirmed

\label{fig:qjep_paths}
\end{figure} 

For trials in which the model does a worse job of predicting (accuracy <50$\%$) we can see that the human behaviour appears less organised in terms of proximity in general and it seems unlikely that adding some form of path-length optimisation to our model would improve accuracy with these participants. We can see this more clearly in Figure \ref{fig:qjep_path_lengths}: while our model does a good job of capturing the average total path length, there is considerable variation with some participants generating shorter paths and some longer. We can see  that this appears to be systematically related to how well our model can predict behaviour. Interestingly, in some cases, poorly predicted trials appear to have very strong \textit{pS} biases, collecting all exemplars of one target and then the other: thus, the participants are carrying out a strategy that the model is not capturing well. This fits with conjunction trials being harder for the model to predict, as these trials generally have fewer switches. 
\vspace{-10pt}
\begin{figure}[H]
\includegraphics[width=13.5 cm]{Figures/path_length.pdf}
\caption{({\textbf{\hl{a}}}) Histogram showing the distribution of the median path lengths for our human participants and model (fitted to each human participant via the random effect structure). (\textbf{\hl{b}}) The relationship between model accuracy and the proportion of trials in which the human participant has a shorter total path length than predicted.}%MDPI:please divide the figure into 2 subfigures and  note them accordingly. Subfigures should be numbered by Latin letters with parentheses, e.g., (a), (b), (c), or (A), (B), (C), etc., which should be put below the image or within the image; only Latin letters should be bold; parentheses should be whole. Subfigures should be provided with sub-caption and put sub-caption in figure caption. RESPONSE: Done
%MDPI: Please confirm that all the figures are the original work of this manuscript’s authors. If they have been adapted or they belong to another author/publisher, it’s required to send us the copyright agreement for using it. Thank you for your cooperation! RESPONSE: Confirmed

\label{fig:qjep_path_lengths}
\end{figure} 

\section{Improving the Model: Location of the First Target Selection \label{s4}}

One weakness of the foraging model by Clarke et al. (2022) \cite{clarke2022foraging} is that it makes little attempt to predict which item will be selected first. In this section we aim to improve this by modelling participant bias (or preference) in their choice of initial target selection. Our approach is inspired by previous work on the central bias in scene viewing \cite{clarke2014deriving, clarke2017}. These studies fit truncated Gaussian distributions to the  $(x,y)$ coordinates of fixation locations. Here we will do similar, and fit beta distributions \hl{(}We use beta distributions here as it is difficult to specify a mixture model using truncated Gaussians in Stan. However, beta distributions may well be a more appropriate choice\hl{)} %MDPI: footer is not allowed in this type article, move it to the (), please confirm. RESPONSE: Confirmed.
to locations of the initial target selections in the visual foraging task. 

Figure \ref{fig:qjep_init_sel_hex} shows the distribution of the locations of first target selections. We can see that unlike the central bias in fixations during scene viewing, the distribution of initial target locations appear to be bimodal: while most targets are positioned in the top left hand corner, there is a second smaller distribution of central target selections. This appears to be due to different participants choosing to utilise different strategies rather than within-subject variation. However, we have no evidence that these different initial strategies affect the model parameters in any systematic way (see {\hl{Supplementary Materials: Part 1}}). We try two different modelling approaches (multilevel and mixture) to assess whether the less complex mixture model is able to account for the patterns of starting positions seen.

\begin{figure}[H]
\includegraphics[width=12 cm]{Figures/init_sel_hex_plot.pdf}
\caption{Hexagonal heatmaps showing the two-dimensional distribution of the location of initial target selections. Grey areas indicate cells with a count of 0. The left panel shows the distribution over all initial target selections (i.e., multiple trials per participant) while the right panel shows the distribution of each participant's median initial selection (i.e., each participant contributes one data point to the~graph). }%MDPI: Please confirm that all the figures are the original work of this manuscript’s authors. If they have been adapted or they belong to another author/publisher, it’s required to send us the copyright agreement for using it. Thank you for your cooperation! RESPONSE: Confirmed.

\label{fig:qjep_init_sel_hex}
\end{figure} 

\subsection{Multi-Level Modelling Results}

We first consider a multi-level model in which we fit beta distributions to the $x$ and $y$ coordinates of each participant's initial target selections \hl{(}Full details of the analysis can be found in the \hl{Supplementary Materials: Part 1}\hl{)}:%MDPI: footer is not allowed in this type article, move it to the (), please confirm. RESPONSE: Confirmed
\begin{equation}
    x_i \sim Beta(a_{x,i}, b_{x,i})
    \label{eq:beta1x}
\end{equation}
\begin{equation}
    y_i \sim Beta(a_{y,i}, b_{y,i})
    \label{eq:beta1y}
\end{equation}

The results are shown in Figure \ref{fig:qjep_init_sel_mdl} and we can clearly see that most participants either have a strong bias to $x=0$ and $y=0$, or a more diffuse central bias. 
\vspace{-6pt}
\begin{figure}[H]
\begin{adjustwidth}{-\extralength}{0cm}
\centering
\includegraphics[width=17.5 cm]{Figures/init_sel_mdl.pdf}
\end{adjustwidth}
\caption{Posterior fits for ({\textbf{\hl{a}}}) horizontal and ({\textbf{\hl{b}}}) vertical locations of the initial target selection. Each line represents a different participant. ({\textbf{\hl{c}}}) A scatter plot showing the $x$ and $y$ posterior means for each participant.}%MDPI: please divide the figure into 3 subfigures and  note them accordingly. Subfigures should be numbered by Latin letters with parentheses, e.g., (a), (b), (c), or (A), (B), (C), etc., which should be put below the image or within the image; only Latin letters should be bold; parentheses should be whole. Subfigures should be provided with sub-caption and put sub-caption in figure caption. RESPONSE: Done
%MDPI: Please confirm that all the figures are the original work of this manuscript’s authors. If they have been adapted or they belong to another author/publisher, it’s required to send us the copyright agreement for using it. Thank you for your cooperation! RESPONSE: Confirmed

\label{fig:qjep_init_sel_mdl}
\end{figure} 

\subsection{Mixture Modelling}

While the multi-level model outlined above appears to offer a good fit for the between-subject differences in initial target selection location, it comes at the expense of requiring four parameters per participant (= 290 for the 58 participants in our dataset). Given that the majority of our participants appear to be following one of two distinct strategies, we can potentially simplify our model by using a two-component mixture model:
\clearpage
\begin{equation}
    x_i \sim \lambda_i \times Beta(a_{x,1}, b_{x,1}) + (1-\lambda_i) \times Beta(a_{x,2}, b_{x,2})
    \label{eq:beta2x}
\end{equation}
\begin{equation}
    y_i \sim \lambda_i \times Beta(a_{y,1}, b_{y,1}) + (1-\lambda_i) \times Beta(a_{y,2}, b_{y,2})
    \label{eq:beta2y}
\end{equation}

This reduces the number of parameters to eight (to specify the two Beta distributions) and then one $\lambda$ value per participant (= 66 in our dataset). While the formulae in Equations \eqref{eq:beta2x} and \eqref{eq:beta2y} appear more complex than those in Equations \eqref{eq:beta1x} and \eqref{eq:beta1y}, the model requires less than a quarter as many free parameters. 

The model fit is shown in Figure \ref{fig:qjep_init2_sel_mdl}. We can see that the two components identified by the model clearly correspond to the top corner and diffuse-central strategies discussed above. We can also see from the lambda values that our participants take on a range of different mixtures between the two. 
\vspace{-12pt}
\begin{figure}[H]
\begin{adjustwidth}{-\extralength}{0cm}%\centering
\centering
\includegraphics[width=1.3\textwidth]{Figures/init_sel2_mdl.pdf}
\end{adjustwidth}
\caption{Posterior fits for the ({\textbf{\hl{a}}}) horizontal and ({\textbf{\hl{b}}}) vertical locations of the initial target selection. ({\textbf{\hl{c}}}) shows the mixing parameters for each participant, with the error bar indicating the $95\%$ HDPI.}%MDPI:Please define "x" and "y" from subfigures 1 and 2. Also, please divide the figure into 3 subfigures. Subfigures should be numbered by Latin letters with parentheses, e.g., (a), (b), (c), or (A), (B), (C), etc., which should be put below the image or within the image; only Latin letters should be bold; parentheses should be whole. Subfigures should be provided with sub-caption and put sub-caption in figure caption. RESPONSE: Done
%MDPI: Please confirm that all the figures are the original work of this manuscript’s authors. If they have been adapted or they belong to another author/publisher, it’s required to send us the copyright agreement for using it. Thank you for your cooperation! RESPONSE: Confirmed

\label{fig:qjep_init2_sel_mdl}
\end{figure} 

\subsection{Posterior Predictions}

To compare the two approaches, we assess how well they fit the empirical data: we can use the fitted distributions to calculate the weight assigned to the selected target by each method for each trial in our dataset (\hl{Figure} \ref{fig:qjep_init_sel_pred}). We can see that both models give a similar distribution of target weights and accuracies. Overall, both methods  select the correct target in around half of the trials. Interestingly, as above, there is considerable variation between participants, with participants who favour the top-left corner being easier to predict. %MDPI: Please move Figure 8 right after this paragraph ends(Figures and tables need to appear in exact numerical order, right after their first citation) RESPONSE: Done

\begin{figure}[H]
\includegraphics[width=13.8 cm]{Figures/qjep_init_sel_preds.pdf}
\caption{({\textbf{\hl{a}}}): a count of initial weight values predicted from both methods (mixture and multi-level). ({\textbf{\hl{b}}}): the proportion of times the first selection was correct for both methods.  ({\textbf{\hl{c}}}): as lambda (the model parameter underlying initial weight) increases, accuracy increases i.e., the model is more accurate for the cases where participants are selecting a corner as their initial target selection. The dots each represent an individual participant, and the line is the line of best fit.}% MDPI: please divide the figure into 2 subfigures and  note them accordingly. Subfigures should be numbered by Latin letters with parentheses, e.g., (a), (b), (c), or (A), (B), (C), etc., which should be put below the image or within the image; only Latin letters should be bold; parentheses should be whole. Subfigures should be provided with sub-caption and put sub-caption in figure caption.
%MDPI: Please provide a short explanation for the black dots from the 3rd subfigure RESPONSE: Done
%MDPI: Please confirm that all the figures are the original work of this manuscript’s authors. If they have been adapted or they belong to another author/publisher, it’s required to send us the copyright agreement for using it. Thank you for your cooperation! RESPONSE: Confirmed


\subsection{Replication} 
%mdpi: ref citation can not be in the section title, please move it to the main text. RESPONSE: Done.
To test how well our methods generalise, we used the dataset from \citep{kristjansson2014}. We split this into a training dataset (50\% of the original data i.e., 10 trials per participant) and a test dataset (the remaining 50\% of the data). The foraging model is fit on the training dataset, and the parameters from this model are then used to predict behaviour at the level of individual trials. We show that our methods appear to generalise beyond \citep{clarke2022}: in particular, feature foraging was more predictable than conjunction foraging, and the model is well calibrated, with participants more frequently selecting targets with higher model assigned weights. Proximity also seems to be an important factor in determining model accuracy, at least for the feature condition, although the relatively low number of participants in this experiment makes it harder to draw strong conclusions. One interesting difference compared to \citep{clarke2022} is in initial target selection: while there was a strong bias towards starting in a corner, there was little evidence of central bias, perhaps because this experiment was completed on an iPad. Further details and graphs can be found in the \hl{Supplementary Materials: Part 2}.




\label{fig:qjep_init_sel_pred}
\end{figure} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion}

In our original model of visual foraging \citep{clarke2022foraging}, we were able to robustly measure aggregate parameters that underlie behaviour, at both an overall mean level across an experiment, and at the level of individual participants. In the current manuscript, we asked a more difficult question: could this model make target-by-target predictions, guessing which target would be picked next within a trial? We found that for some trials our model was over 80\% accurate, demonstrating that at least in some cases, our model is able to make good predictions of target behaviour.

A benefit of our approach is that it is relatively easy to interrogate the model to try to understand why prediction accuracy is higher for some participants and trials than others. The model is generally better at feature search compared to conjunction search, and also seems to be better at predicting participants with a stronger proximity bias. The model also seems to assign more weight to 'runner-up' items than real humans do. Real foragers thus seem to seriously consider only a few nearby targets (although they do still have some low probability of selecting a further away target), in contrast to the model, in which the item weights assigned fall off more gradually with distance.

However, we can also delve deeper into exploratory hypotheses about other factors that might influence predictability. One suggestion is that the people who our model is able to predict are in some way the most 'optimal'. If this is the case, we might expect that model predictability should correlate with how good people are at the task. There is some evidence that on a trial-by-trial basis, shorter trials are easier for the model to predict, and on a target-by-target basis, faster moves from one target to the next are more easily predicted by the model (see \hl{Supplementary Materials: Part 1}). This likely is linked to the fact that our model is better at predicting participants with a stronger proximity bias, as targets that are closer to the previous target are also likely to be selected more quickly. Intuitively, it makes sense that the model makes poorer predictions when a participant has exhausted a local patch and may need to make a bigger jump to a more distant area to carry on collecting targets, as there may be multiple candidates and distance may no longer be such a good predictor (for example, people may choose to move to the next most densely populated patch). 
 
There has been some previous work on within-trial behaviour in the context of foraging. For example, \citep{tagu2020} found that intertarget times vary across a trial (with targets selected later in the trial being slower) and intertarget distances also vary (with targets selected later in the trial having greater intertarget distances). Similarly, \citep{thornton2022foraging} showed that switches between target categories can be characterised by a change in foraging 'tempo', and if participants are forced to forage at a particular speed, by asking them to synchronise with an auditory signal, higher tempos led to a systematic decrease in the probability of participants switching between target categories. In the current version of the model, we do not consider time explicitly (although distance is likely to be correlated with time in this task). However, it would be possible to extend the model to incorporate timing information, and this may help to improve the target-by-target predictions.
 
From inspection of Figure \ref{fig:qjep_paths}, we noticed that in some cases, participants may be using a type of local path minimisation procedure, selecting a further away target in order to minimise overall path length, which the model does not always seem to predict. Could this help to explain some of the mistakes the model makes? Overall, the model does not seem to make substantially longer paths through the items than the human participants. However, there is a relationship between model accuracy and the proportion of times that participants select paths shorter than the model: as model accuracy increases, people seem to be more likely to have paths shorter than the model predicts. This seems to suggest that path minimisation is not a good explanation for the participants our model is failing to capture, and instead the people that are difficult to predict are taking longer paths than the model thinks they should take.

Our exploratory analyses seem to hint at the idea that the people who are difficult for the model to predict are those who do not behave in a manner that optimises path length. It could be that these participants are simply unpredictable, and do not behave in a consistent manner on a target-by-target basis, making it difficult for a model to predict their behaviour at this level of granularity. However, it is possible that there are other factors that we have not taken into account that could further help explain these discrepancies. Participants who 'stick' to one type of target are likely to have longer path lengths, and it seems that the model does do a poorer job of predicting conjunction trials, which generally have stronger \textit{pS} biases. Thus, one area for future improvement of the model is to try to capture this behaviour better: it may be that a 'floor' parameter (on a participant-by-participant basis) in the spatial fall off, allocating more weight to more distant targets, may help account for currently difficult to predict trial behaviour.

There are also many other factors that the model does not account for. For example, it is possible that relatively unpredictable participants use multiple rules and switch between them in ways that we are not currently captured in the model. Some participants may also lose track of where they are more frequently (perhaps due to attentional lapses) and thus may show foraging patterns that are predictable apart from unpredictable discontinuities, which again may not be easy to model. Another possibility is that participants may make exploratory, information-seeking visits to different parts of the array due to uncertainty about the targets present: this may be particularly important for conjunction trials, where peripheral visual information about shape is likely to be poorer. Finally, the displays had artificial discontinuities (i.e., screen edges) which may lead to different behaviour in different participants: some may carry out the task as if they are reading, reaching the right hand edge and then swinging back to the left, whereas others may 'bounce' off an edge they have just reached. Our model may well be able to predict the latter, but make less good predictions for the former behaviour.

To a large extent, initial item selection strategy seems to be stable within a participant, with people either starting in the corner or the centre of the screen. Our model is therefore also relatively accurate for selecting the initial target, selecting the correct target in around half of the trials (with a mixture model performing similarly to a multilevel model with many more parameters). Participants who prefer the top left corner were easiest to predict, probably because these participants formed the majority group in this experiment. Our results are congruent with \citep{kristjansson2022moving}, who found a bottom left bias in a 3D environment. They suggest this may be an advantageous strategy because it allows your foraging to be more organised (e.g., in an 'S-shape', as has been found in real world search tasks \citep{riggs2017importance}). The smaller group of participants who prefer to start in the centre of the screen may be displaying similar behaviour to the well-known central bias in eye movements \citep{tatler2007central, tseng2009quantifying, clarke2014deriving} However, it is worth noting that these findings probably depend strongly on the context of the task. For example, there was no obvious central bias subgroup in \citep{kristjansson2014}, possibly reflecting the fact that this experiment was carried out on an iPad, compared to the computer screen and mouse used in \citep{clarke2022}. One challenge for future modelling work is to what extent we should try to account for these types of task-specific details: by incorporating them, we can generate better predictions for the experiment we are currently modelling, but perhaps at the cost of generalisability to other paradigms.

One of the key benefits of computational models is that they allow us to rigorously test how well we are able to predict behaviour, and can provide insights into what factors we are not capturing that may have important influences on how participants complete a task. We have demonstrated that our foraging model \cite{clarke2022foraging} is not only able to predict behaviour in aggregate, but can also make reasonable predictions at the target-by-target level. It is particularly good if a participant has a strong proximity bias, and on 'feature' trials where participants are normally switching between target types fairly frequently. It is also good at predicting where participants will start on a trial. The model finds it more difficult to predict 'discontinuous' jumps, which could be caused by a range of factors: a more difficult foraging task (e.g., conjunction searches), local path length minimisation, inattention, or aspects of the physical search space (e.g., edges of the screen). However, it would be possible to extend the model to incorporate these factors e.g., by incorporating a heuristic that would allow for local path minimisation, or introducing time as a predictor in the model. Our findings also suggest possible future directions for empirical work, such as evaluating the effect of inattention on foraging behaviour, or how the foraging targets are organised in space. We suggest that computational modelling is a powerful tool for helping us to understand behaviour, both by incorporating previous research into a shared framework, and by making testable predictions for future work.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{6pt} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% optional
\supplementary{\hl{The following are available online at} \linksupplementary{s1}, Document S1: Supplementary Materials 1, Document S2: Supplementary Materials 2}
%mdpi: please fully complete this part with information regarding  supplementary materials, as shown in the above example. RESPONSE: Done

% Only for the journal Methods and Protocols:
% If you wish to submit a video article, please do so with any other supplementary material.
% \supplementary{The following are available at \linksupplementary{s1}, Figure S1: title, Table S1: title, Video S1: title. A supporting video article is available at doi: link.} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\authorcontributions{Conceptualization, A.D.F.C., A.R.H., and A.E.H.; methodology, A.D.F.C.; software, A.D.F.C.; formal analysis, A.D.F.C.; data curation, A.D.F.C. and A.E.H.; writing---original draft preparation, A.D.F.C.; writing---review and editing, A.R.H. and A.E.H.; visualization, A.D.F.C. and A.E.H.; funding acquisition, A.D.F.C. and A.R.H. \hl{All authors} have read and agreed to the published version of the manuscript.}%MDPI: Please make sure all the authors were mentioned in this section RESPONSE: Confirmed

\funding{This research was funded by the Economic and Social Research Council grant number ES/S016120/1 to A.D.F.C. and A.R.H.}

\institutionalreview{Not applicable (secondary data analysis only).}

\informedconsent{Not applicable (secondary data analysis only).}

\dataavailability{Data supporting reported results can be found on Github at \url{https://github.com/scienceanna/foraging_svg}. The datasets analysed in this study can be found at \url{https://osf.io/y6qbv/}, \hl{accessed on 25th April 2022}, and \linebreak  \url{https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0100752}, \hl{accessed on 27th May 2022}.} 
%MDPI: Please add the access date (Format: Date Month Year). e.g., (accessed on 1 January 2020). RESPONSE: Done.


\acknowledgments{\hl{The authors would like to thank all the researchers who publicly shared their data.}}%mdpi: In this section you can acknowledge any support given which is not covered by the author contribution or funding sections. This may include administrative and technical support, or donations in kind (e.g., materials used for experiments). RESPONSE: Done.

\conflictsofinterest{The authors declare no conflict of interest.} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{adjustwidth}{-\extralength}{0cm}
%\printendnotes[custom] % Un-comment to print a list of endnotes

\reftitle{References}
\begin{thebibliography}{999}

\bibitem[Dawkins(1971)]{dawkins1971}
Dawkins, M.
\newblock Shifts of ‘attention’in chicks during feeding.
\newblock {\em Anim. Behav.} {\bf 1971}, {\em 19},~575--582.

\bibitem[Kristj{\'a}nsson \em{et~al.}(2014)Kristj{\'a}nsson, J{\'o}hannesson,
  and Thornton]{kristjansson2014}
Kristj{\'a}nsson, {\'A}.; J{\'o}hannesson, {\'O}.I.; Thornton, I.M.
\newblock Common attentional constraints in visual foraging.
\newblock {\em PLoS ONE} {\bf 2014}, {\em 9},~e100752.

\bibitem[Charnov(1976)]{charnov1976}
Charnov, E.L.
\newblock Optimal foraging, the marginal value theorem.
\newblock {\em Theor. Popul. Biol.} {\bf 1976}, {\em 9},~129--136.

\bibitem[Bartumeus \em{et~al.}(2005)Bartumeus, Da~Luz, Viswanathan, and
  Catalan]{bartumeus2005}
Bartumeus, F.; Da~Luz, M.; Viswanathan, G.; Catalan, J.
\newblock Animal search strategies: A quantitative random-walk analysis.
\newblock {\em Ecology} {\bf 2005}, {\em 86},~3078--3087.
\newblock 
  {{https://doi.org/10.1890/04-1806}}.

\bibitem[Benhamou(2007)]{benhamou2007}
Benhamou, S.
\newblock How many animals really do the Lévy walk?
\newblock {\em Ecology} {\bf 2007}, {\em 88},~1962--1969.
\newblock  {{https://doi.org/10.1890/06-1769.1}}.

\bibitem[Dukas and Ellner(1993)]{Dukas1993}
Dukas, R.; Ellner, S.
\newblock Information processing and prey detection.
\newblock {\em Ecology} {\bf 1993}, {\em 74},~1337--1346.

\bibitem[Clarke \em{et~al.}(2022)Clarke, Hunt, and Hughes]{clarke2022foraging}
Clarke, A.D.; Hunt, A.R.; Hughes, A.E.
\newblock Foraging as sampling without replacement: A Bayesian statistical
  model for estimating biases in target selection.
\newblock {\em PLoS Comput. Biol.} {\bf 2022}, {\em 18},~e1009813.

%MDPI: Refs. 8 and 16 are duplicated. Please remove ref. 16 and rearrange all the references to appear in numerical order. Please ensure that there are no duplicated references.
%MDPI: newly added, please confirm. RESPONSE: The 2022 reference seems to be the correct one - now corrected.

\bibitem[Clarke \em{et~al.}(2022)Clarke, Irons, James, Leber, and
  Hunt]{clarke2022}
Clarke, A.D.; Irons, J.L.; James, W.; Leber, A.B.; Hunt, A.R.
\newblock Stable individual differences in strategies within, but not between,
  visual search tasks.
\newblock {\em Q. J. Exp. Psychol.} {\bf 2022}, \hl{\emph{75},  289--296.}
%MDPI: newly added, please confirm. RESPONSE: Confirm

\bibitem[Tagu and Kristj{\'a}nsson(2021)]{tagu_kristjansson_2021}
Tagu, J.; Kristj{\'a}nsson, {\'A}.
\newblock The selection balance: Contrasting value, proximity and priming in a
  multitarget foraging task. \emph{\hl{Cognition}}  \textbf{2021}, \hl{\emph{218}, 104935}.
\newblock {{https://doi.org/10.31234/osf.io/48pzy}}.
%MDPI: newly added, please confirm. RESPONSE: Confirm


\bibitem[Tagu and Kristj{\'a}nsson(2020)]{tagu2020}
Tagu, J.; Kristj{\'a}nsson, {\'A}.
\newblock Dynamics of attentional and oculomotor orienting in visual foraging
  tasks.
\newblock {\em Q. J. Exp. Psychol.} {\bf 2022}, \hl{\emph{75}, 260--276.}
%MDPI: newly added, please confirm. RESPONSE: Confirm


\bibitem[Clarke and Tatler(2014)]{clarke2014deriving}
\hl{Clarke, A.D.; Tatler, B.W.} %MDPI: Refs. 12 and 15 are duplicated. Please remove ref. 15 and rearrange all the references to appear in numerical order. Please ensure that there are no duplicated references. RESPONSE: Fixed
\newblock Deriving an appropriate baseline for describing fixation behaviour.
\newblock {\em Vis. Res.} {\bf 2014}, {\em 102},~41--51.

\bibitem[Clarke \em{et~al.}(2017)Clarke, Stainer, Tatler, and Hunt]{clarke2017}
Clarke, A.D.; Stainer, M.J.; Tatler, B.W.; Hunt, A.R.
\newblock The saccadic flow baseline: Accounting for image-independent biases
  in fixation behavior.
\newblock {\em J. Vis.} {\bf 2017}, {\em 17},~12.

\bibitem[{Stan Development Team}(2020)]{rstan}
{Stan Development Team}.
\newblock {RStan}: The {R} interface to {Stan}. 
\newblock \hl{R package version 2.21.2. 2020. } %MDPI: Please provide more information about the type of the article, such as book (please provide the name of the publisher and location of it), online resource (please provide the URL of the website and the date it was accessed (day, month, year)), or journal article (please provide the name of the journal, the year and volume it published, and the page number). RESPONSE: This is software: the URL is https://mc-stan.org/.

\bibitem[Thornton \em{et~al.}(2022)Thornton, Nguyen, and
  Kristj{\'a}nsson]{thornton2022foraging}
\hl{Thornton, I.M.; Nguyen, T.T.; Kristj{\'a}nsson, {\'A}}.
\newblock Foraging tempo: Human run patterns in multiple-target search are
  constrained by the rate of successive responses.
\newblock {\em Q. J. Exp. Psychol.} {\bf 2022}, {\em
  75},~297--312.

\bibitem[Kristj{\'a}nsson \em{et~al.}(2022)Kristj{\'a}nsson, Draschkow,
  P{\'a}lsson, Haraldsson, J{\'o}nsson, and
  Kristj{\'a}nsson]{kristjansson2022moving}
Kristj{\'a}nsson, T.; Draschkow, D.; P{\'a}lsson, {\'A}.; Haraldsson, D.;
  J{\'o}nsson, P.{\"O}.; Kristj{\'a}nsson, {\'A}.
\newblock Moving foraging into three dimensions: Feature-versus
  conjunction-based foraging in virtual reality.
\newblock {\em Q. J. Exp. Psychol.} {\bf 2022}, {\em
  75},~313--327.

\bibitem[Riggs \em{et~al.}(2017)Riggs, Cornes, Godwin, Liversedge, Guest, and
  Donnelly]{riggs2017importance}
Riggs, C.A.; Cornes, K.; Godwin, H.J.; Liversedge, S.P.; Guest, R.; Donnelly,
  N.
\newblock The importance of search strategy for finding targets in open
  terrain.
\newblock {\em Cogn. Res. Princ. Implic.} {\bf 2017},
  {\em 2},~1--17.

\bibitem[Tatler(2007)]{tatler2007central}
Tatler, B.W.
\newblock The central fixation bias in scene viewing: Selecting an optimal
  viewing position independently of motor biases and image feature
  distributions.
\newblock {\em J. Vis.} {\bf 2007}, {\em 7},~4.

\bibitem[Tseng \em{et~al.}(2009)Tseng, Carmi, Cameron, Munoz, and
  Itti]{tseng2009quantifying}
Tseng, P.H.; Carmi, R.; Cameron, I.G.; Munoz, D.P.; Itti, L.
\newblock Quantifying center bias of observers in free viewing of dynamic
  natural scenes.
\newblock {\em J. Vis.} {\bf 2009}, {\em 9},~4.

\end{thebibliography}

\end{adjustwidth}
\end{document}

