---
title: 'SVG: The Paper (2nd part of supplementary material)'
author: "Alasdair Clarke, Amelia Hunt &  Anna Hughes"
date: "13/04/2022"
output:
  tufte::tufte_html:
    number_sections: true
    fig_height: 4
    bibliography: literature.bib
    link-citations: yes  
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
options(digits = 3)
set.seed(2022)
```


```{r, message=FALSE}
# attach packages
library(tidyverse)
library(patchwork)
library(tidybayes)
library(ggrepel)
library(truncnorm)
```

```{r}
# set ggplot2 options
theme_set(ggthemes::theme_tufte() + theme(plot.background = element_rect(fill = "#fffff8", colour = "#fffff8")))

# some more colours
col3 = c("#d04e00", "#ddc000","#34b6c6",  "#79ad41", "#4063a3")
                
our_cols <- c("#1b9e77", "#7570b3")
options(ggplot2.discrete.fill = our_cols ,
        ggplot2.discrete.colour = our_cols)
```

Terminology questions:
- item v target
- participant v observer

# Trial Level Posterior Predictions{#label}

```{r echo = FALSE}
source("functions/get_model_params.R")
source("functions/compute_weights.R")
```

This is the replication with Kristjansson (2014) data.

```{r, echo = TRUE}
# read in data
d <- read_csv("data/test_arni.csv", show_col_types = FALSE) %>%
  mutate(condition = as_factor(condition),
         condition = fct_recode(condition, feature = "1", conjunction = "2"),
         targ_type = as_factor(targ_type))
```

Here, we have used half the data as a training set which we used to fit the initial model. This is then used for computing model weights for the test dataset.

For simplicity's sake, we will characterise all of our estimated posterior probability densities with just the mean value. This gives us four parameters per participant: $b_S, b_A, b_P, b_D$. 

Using these fitted values, we can now step through each target selection in the dataset and computer the probability weights our model assigns to each remaining target. For each target selection, we record:

- the item assigned the highest weight by our model.
- whether this item was then selected by the human participant. 


```{r echo = TRUE}
# item weights pre-computed - see scripts/extract_item_weights_using_model.R
a <- readRDS("scratch/kristjansson_model_weights_train.rda") %>%
  filter(found != 1) # remove initial selections

# compute the average weight for each participant x condition x item selection
# also compute the proportion of times in which the item selected by the 
# participant was judged the most likely to be selected by our model.
a %>% group_by(condition, observer, found) %>%
  summarise(meanb = mean(b),
            prop_best = mean(selected_max), .groups = "drop") %>%
  # finally, also compute what we would expect under a null-model
  mutate(chance = 1/(41-found))-> a_agg
```

We can then plot this data to show how our ability to predict which target will be selected next varies throughout a trial. We can see a difference between *feature* and *conjunction* foraging, with *feature* foraging being more predictable (though the distinction is less clear after the initial 20 target selections).

```{r, fig.cap = "(*left*) The average weight assigned to each selected target by our model. (*right*) The proportion of trials the item with the largest assigned weight was selected by the participant.  Each dot shows data from an individual participant in a condition and the shaded region indicates the interval in which we expect 67% of participants to fall."}

# plot target selected weights
ggplot(a_agg, aes(x = found, y = meanb, colour = condition, fill = condition)) + 
  geom_jitter(data = filter(a_agg, found<40), width = 0.1, height = 0, alpha = 0.2) + 
  stat_lineribbon(.width = 0.67, alpha = 0.50) +
  geom_path(data = filter(a_agg, observer == 1, condition == "feature"), 
                          aes(y = chance), linetype = 2, colour = "black") + 
  geom_point(data = tibble(x=40, y=1), aes(x, y), size = 1.5, colour = "black", fill = "grey") + 
  scale_x_continuous(breaks = c(2, 10, 20, 40), "target selection", expand = c(0.01, 0.01)) + 
  scale_y_continuous("average weight from model", expand = c(0.01, 0.01)) -> plt_b

ggplot(a_agg, aes(x = found, y = prop_best, colour = condition, fill = condition)) + 
  geom_jitter(data = filter(a_agg, found<40), width = 0.1, height = 0.00, alpha = 0.2) + 
  stat_lineribbon(.width = 0.67, alpha = 0.50) +
  geom_path(data = filter(a_agg, observer == 1, condition == "feature"), 
            aes(y = chance), linetype = 2, colour = "black") + 
  geom_point(data = tibble(x=40, y=1), aes(x, y), size = 1.5, colour = "black", fill = "grey") + 
  scale_x_continuous(breaks = c(2, 10, 20, 40), "target selection", expand = c(0.01, 0.01)) + 
  scale_y_continuous("proportion most likely was selected", expand = c(0.01, 0.01)) -> plt_c

plt_b + plt_c + plot_layout(guides = "collect")  &
  theme(legend.position = 'bottom',
        legend.direction = 'horizontal')

rm(plt_c)

```


## Calibration

Is our model calibrated? By this we mean, if our model assigns an item a probability $p$ of being selected next, then is it actually selected (by the human participant) this often?

We will calculate this as follows: for each target selection (on each trial, for each participant) we look at the weight assigned to the most likely item, and then look at whether it was selected or not. 


```{r, echo = TRUE}
my_breaks <- seq(0.0, 1, 0.05)

a %>% mutate(b_bin = cut(max_b, breaks = my_breaks, labels = FALSE)) %>%
  group_by(condition, b_bin) %>% 
  summarise(acc = mean(selected_max), .groups = "drop") %>%
  mutate(b_bin = as.numeric(b_bin)/(length(my_breaks)-1)) -> a_cut
```

We can see from the Figure below that our model is well-calibrated (at least in terms of the target with the highest assigned weight). If we repeat this calculation for the 2nd and 3rd ranked candidate items we can see that we slightly underweight these items, but we are still pretty accurate. 

```{r, fig.height=3.5, fig.cap = "(*left*) Calibration plot for our foraging model. The *x*-axis gives the largest weight assigned by the model while the *y*-axis shows how often that target was actually selected by a human participant. (*right*) This plot shows how often the 2nd and 3rd ranked items are selected based on the weights assigned by the model."}

ggplot(a_cut, aes(b_bin, acc, colour = condition, fill = condition, shape = condition)) + 
  geom_point() + 
  geom_line(stat = "smooth", method = "loess", 
            formula = y ~ x, alpha = 0.65, size = 2, se = F) +
  geom_abline(linetype = 2) +
  scale_x_continuous("largest item weight") +
  scale_y_continuous("proportion of times selected") +
  theme(legend.position = "bottom") -> plt1

a %>% select(max_b2, max_b3, selected_max2, selected_max3) %>%
  unite(`2`, max_b2, selected_max2) %>%
  unite(`3`, max_b3, selected_max3) %>%
  pivot_longer(c(`2`, `3`), names_to = "rank", values_to = "b") %>%
  separate(b, into = c("b", "selected"), sep = "_", convert = TRUE) %>%
  mutate(b_bin = cut(b, breaks = my_breaks, labels = FALSE)) %>%
  group_by(b_bin) %>% 
  summarise(acc = mean(selected), .groups = "drop") %>%
   mutate(b_bin = as.numeric(b_bin)/(length(my_breaks)-1)) %>%
  filter(is.finite(acc)) %>%
  ggplot(aes(b_bin, acc)) + 
  geom_point() + 
  geom_line(stat = "smooth", method = "loess", 
            formula = y ~ x, alpha = 0.65, size = 2, se = F) +
  geom_abline(linetype = 2) +
  scale_x_continuous("runner up item weights") +
  scale_y_continuous("proportion of times selected") -> plt2

 plt1 + plt2

#ggsave("../Figures/qjep_preds.pdf", 
#       plt_b + plt1 + plt2 + 
#         plot_layout(guides = "collect") & theme(legend.position = "bottom"),
#       width = 10, height = 4)

```

```{r}
rm(a_cut, my_breaks, plt1, plt2)
```

### Individual Differences

How often does each participant select the target with the largest weight? 

```{r, fig.height=3.5, fig.cap = "Prediction scores for participants. Boxplots show quartile range and the grey lines indicate individual participants. (The dots indicate outliers.)", echo= TRUE}

a_agg %>% group_by(observer, condition) %>%
  summarise(accuracy = mean(prop_best), .groups = "drop") -> a_acc

ggplot(a_acc, aes(x= condition, y = accuracy, fill = condition)) + 
  geom_boxplot() +
  geom_line(aes(group = observer), alpha = 0.25, colour = "sienna4") +
  scale_y_continuous("model accuracy",  
                     limits = c(0.35, 0.8), breaks = seq(0.3, 0.8, 0.1)) -> plt_a

plt_a
```
Model accuracy is quite high (though lower than for Clarke et al).


Is this explained by differences in proximity weighting? i.e., are participants with weaker proximity biases harder to predict? Yes! 

```{r fig.height = 6, fig.cap = "Accuracy of our model varies with the strength of an individual's model parameters. We can see two clear outlier participants (marked with an X). "}
# computed in xxxx.R
fit <- readRDS("scratch/kristjansson_model_fit_train.rda") %>%
  full_join(a_acc, by = c("observer", "condition")) %>%
  mutate(is_outlier = if_else(observer %in% c(35, 45), TRUE, FALSE))


fit_plt <- pivot_longer(fit, c(pA, pS, bP, bM), 
                     names_to = "parameter", values_to = "value") 

ggplot(fit_plt, aes(value, accuracy, colour = condition)) + 
  geom_point(aes(shape = is_outlier)) +
  geom_smooth(data = filter(fit_plt, is_outlier == 0), method = "lm", formula = y ~ x, se = FALSE) + 
  scale_shape_manual(values = c(19, 4), guide = "none") + 
  facet_wrap(~parameter, scales = "free") +
  theme(legend.position = "bottom")

plt_b <- ggplot(filter(fit_plt, parameter == "bP"), 
                aes(value, accuracy, colour = condition)) + 
  geom_point(aes(shape = is_outlier)) +
  geom_smooth(data = filter(fit_plt, is_outlier == 0, parameter == "bP"), method = "lm", formula = y ~ x, se = FALSE) + 
  scale_shape_manual(values = c(19, 4), guide = "none") + 
  theme(legend.position = "none") + 
   scale_y_continuous("model accuracy",  
                     limits = c(0.35, 0.8), breaks = seq(0.3, 0.8, 0.1))

#ggsave("../Figures/qjep_indiv_diff.pdf", 
#       plt_a + plt_b + 
#         plot_layout(guides = "collect") & theme(legend.position = "right"), width = 8, height = 4)
```

We can see that individual differences in proximity tuning account for nearly all of the differences in predictability from person to person, and also between the *feature* and *conjunction* conditions.

```{r, echo = TRUE}
dm <- filter(fit, is_outlier == 0) # have not currently defined anything as an outlier

summary(lm(accuracy ~ condition * bP, 
           data = dm))
```

We can see that the difference in the proximity parameter between conditions appears to account for most of the variables in our model's accuracy from one participant to the next. 

We can also see if the other parameters have an effect - which they do seem to.


```{r echo=TRUE}
summary(lm(accuracy ~ pA + pS + bM + bP, 
           data = dm))
```

# Initial Selection

Similar to Clarke et al, there is a bias towards starting in a corner. Interestingly, there isn't much evidence of a central bias (possibly because these trials were completed on an iPad).

```{r echo = TRUE}
# Reimport data again (as we removed the initial target selection earlier)
d <- read_csv("data/test_arni.csv", show_col_types = FALSE) %>%
  filter(found == 1) %>%
  mutate(condition = as_factor(condition))
```

```{r fig.height = 3.5, fig.cap="(*left*) Density plot for initial target selections, over all participants and all trials. (*right*) Median $x$ and $y$ coordinates for the initial target selection for each participant."}
my_theme <- theme(axis.line=element_blank(),axis.text.x=element_blank(),
          axis.text.y=element_blank(),axis.ticks=element_blank(),
          axis.title.x=element_blank(),
          axis.title.y=element_blank(),legend.position="none",
          panel.background=element_rect(color  = "#808080", size = 2,linetype = 'dotted', fill ="#808080"),
        panel.border= element_rect(colour = "black", fill=NA, size=5),
        panel.grid.major=element_blank(),
        panel.grid.minor=element_blank())

ggplot(d, aes(x, y)) + geom_hex(aes(colour = ..count..), bins = 12) +
  scale_fill_viridis_c() + scale_color_viridis_c() +
  my_theme -> plt_all


d %>% group_by(observer, condition) %>% 
  summarise(x = median(x), y = median(y),
            .groups = "drop") %>%
  ggplot(aes(x, y)) + geom_hex(bins = 12) +
  scale_fill_viridis_c()+ scale_color_viridis_c() +
  my_theme -> plt_med

plt_all + plt_med 
#ggsave("../Figures/init_sel_hex_plot.pdf", width = 8, height = 4)

rm(plt_all, plt_med)
```


