---
title: 'SVG: The Paper'
author: "Alasdair Clarke, Amelia Hunt &  Anna Hughes"
date: "13/04/2022"
output:
  tufte::tufte_html:
    toc: TRUE
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
options(digits = 3)
```


```{r, message=FALSE}
# attach packages
library(tidyverse)
library(patchwork)
library(tidybayes)
library(ggrepel)
```


```{r}
# set ggplot2 options
theme_set(ggthemes::theme_tufte() + theme(plot.background = element_rect(fill = "#fffff8", colour = "#fffff8")))

# some more colours
#Isfahan2 = list(c("#d7aca1", "#ddc000", "#79ad41", "#34b6c6", "#4063a3"), c(4, 2, 3, 5, 1), colo
                
our_cols <- c("#1b9e77", "#7570b3")
options(ggplot2.discrete.fill = our_cols ,
        ggplot2.discrete.colour = our_cols)
```


Terminology questions:
- item v target
- participant v observer


# Trial Level Posterior Predictions{#label}

```{r echo = FALSE}
source("functions/get_model_params.R")
source("functions/compute_weights.R")
```

## Clarke et al (2022)

We will start by looking at the visual foraging data from Clarke et al (2022), QJEP. As the current version of our model only takes the $p_A$ bias into account for the initial taret selection, we will ignore these data points for now. 

```{r, echo = TRUE}
# read in data
d <- read_csv("data/clarke_2020_qjep.csv", show_col_types = FALSE) %>%
  mutate(condition = as_factor(condition),
         condition = fct_recode(condition, feature = "1", conjunction = "2"),
         targ_type = as_factor(targ_type))


# item weights pre-computed - see xxxxx.R
a <- readRDS("scratch/qjep_model_weights.rda") %>%
  filter(found != 1) # remove initial selections

# compute the average weight for each participant x condition x item selection
# also compute the proportion of times in which the item selected by the 
# participant was judged the most likely to be selected by our model.
 
a %>% group_by(condition, observer, found) %>%
  summarise(meanb = mean(b),
            prop_best = mean(selected_max), .groups = "drop") %>%
  # finally, also compute what we would expect under a null-model
  mutate(chance = 1/(41-found))-> a_agg
```


```{r, fig.cap = "(*left*) The average weight assigned to each selected target by our model. (*right*) The proportion of trials the item with the largest assigned weight as selected by the participant.  Each dot shows data from an individual particpant in a condition and the shaded region indicates the invertal in which we expect 67% of participants to fall."}

# plot target selected weights
ggplot(a_agg, aes(x = found, y = meanb, colour = condition, fill = condition)) + 
  geom_jitter(data = filter(a_agg, found<40), width = 0.1, height = 0, alpha = 0.2) + 
  stat_lineribbon(.width = 0.67, alpha = 0.50) +
  geom_path(data = filter(a_agg, observer == 1, condition == "feature"), 
                          aes(y = chance), linetype = 2, colour = "black") + 
  geom_point(data = tibble(x=40, y=1), aes(x, y), size = 1.5, colour = "black", fill = "grey") + 
  scale_x_continuous(breaks = c(2, 10, 20, 40), "target selection", expand = c(0.01, 0.01)) + 
  scale_y_continuous("average weight from model", expand = c(0.01, 0.01)) -> plt_b

ggplot(a_agg, aes(x = found, y = prop_best, colour = condition, fill = condition)) + 
  geom_jitter(data = filter(a_agg, found<40), width = 0.1, height = 0.00, alpha = 0.2) + 
  stat_lineribbon(.width = 0.67, alpha = 0.50) +
  geom_path(data = filter(a_agg, observer == 1, condition == "feature"), 
            aes(y = chance), linetype = 2, colour = "black") + 
  geom_point(data = tibble(x=40, y=1), aes(x, y), size = 1.5, colour = "black", fill = "grey") + 
  scale_x_continuous(breaks = c(2, 10, 20, 40), "target selection", expand = c(0.01, 0.01)) + 
  scale_y_continuous("proportion most likely was selected", expand = c(0.01, 0.01)) -> plt_c

plt_b + plt_c + plot_layout(guides = "collect")  &
  theme(legend.position = 'bottom',
        legend.direction = 'horizontal')
ggsave("../Figures/qjep_preds.png", width = 9, height = 4)
```

From this figure we can see a few interesting things to note:

- large individual differences in how well our model can capture and predict behaviour
- feature search is more predictable than conjunction search
- our model appears to be overly conservative

### Calibration

Is our model calibrated? By this we mean, if our model assigns an item a probability $p$ of being selected next, then is it actually be selected (by the human participant) this often?

We will calculate this as follows: for each target selection (on each trial, for each participant) we look at the weight assigned to the most likely item, and then look at whether it was selected or not. 

```{r, echo = TRUE}
n_breaks = 25

a %>% mutate(b_bin = cut(max_b, breaks = n_breaks, labels = FALSE)) %>%
  group_by(condition, b_bin) %>% 
  summarise(acc = mean(selected_max), .groups = "drop") %>%
  mutate(b_bin = as.numeric(b_bin)/n_breaks) -> a_cut
```

We can see from the Figure below that our model is well-calibrated, (at least in terms of the target with the highest assigned weight).

```{r, fig.cap = "Calibration plot for our foraging model. The *x*-axis gives the largest weight assigned by the model while the *y*-axis shows how often that target was actually selected by a human participant."}

ggplot(a_cut, aes(b_bin, acc, colour = condition, shape = condition)) + 
  geom_point() + 
  geom_line(stat = "smooth", method = "loess", 
            formula = y ~ x, alpha = 0.65, size = 2, se = F) +
  geom_abline(linetype = 2) +
  scale_x_continuous("largest item weight") +
  scale_y_continuous("proportion of times selected") + 
  coord_fixed()
```

If anything, our model is a little conservative: items are selected around 5% more often than we'd expect. 

```{r, echo = TRUE}
mean(a_cut$acc - a_cut$b_bin)
```

```{r}
rm(a_cut)
```


We can also repeat this calculate for all other items during/in a trial - is our model accurate/calibrated in cases when 
the participant did not select the item withe highest model weight? 

<TODO>


### Individual Differences

How often does each participant select the target with the largest weight? 

```{r, fig.cap = "Prediction scores for participants. Boxplots show quartile range and the grey lines indicate individual participants. (The dots indicate outliers.)", echo= TRUE}

a_agg %>% group_by(observer, condition) %>%
  summarise(accuracy = mean(prop_best), .groups = "drop") -> a_acc

ggplot(a_acc, aes(x= condition, y = accuracy, fill = condition)) + 
  geom_boxplot() +
  geom_line(aes(group = observer), alpha = 0.25, colour = "sienna4") +
  scale_y_continuous("model accuracy",  
                     limits = c(0.3, 0.8), breaks = seq(0.3, 0.8, 0.1))
```

Is this explained by differences in proximity weighting? i.e., are participants with weaker proximity biases harder to predict? Yes! 

```{r, fig.cap = "Accuracy of our model varies with the strength of an individual's proximity bias."}
# computed in xxxx.R
fit <- readRDS("scratch/qjep_model_fit.rda") %>%
  full_join(a_acc, by = c("observer", "condition"))

ggplot(fit, aes(bP, accuracy, colour = condition)) + 
  geom_point() +
  geom_smooth(method = "lm", formula = y ~ x, se = FALSE)
```
Furthermore, this appears to account for most of the difference between the *feature* and *conjunction* conditions.

```{r, echo = TRUE}
summary(lm(accuracy ~ condition * bP, fit))
```
### Examples of Trials

Let's score every trial by how well the model captures behaviour, and then plot some different examples


```{r, echo = TRUE}
a %>% group_by(observer, condition, trial) %>% 
  summarise(prop_max = mean(selected_max), .groups = "drop") %>% 
  arrange(desc(prop_max)) %>%
  ungroup() -> a_trl
```


```{r, fig.cap = "Histogram of how predictiable indivuidual trials are, defined as how often the model assigned the largest weight to the target that was then selected."}
ggplot(a_trl, aes(prop_max)) + 
  geom_histogram(fill = "#34b6c6", breaks = seq(0, 1, 0.025)) +
  scale_x_continuous("trial predictability")
```

Let's plot some examples oh highly predictable (top 5%), and unpredictable (bottom 5%) trials.




```{r, echo = TRUE}
qs <- quantile(a_trl$prop_max, c(0.05, 0.95))

trls_hard <- filter(a_trl, prop_max < qs[1]) %>% sample_n(3)
trls_easy <- filter(a_trl, prop_max > qs[2]) %>% sample_n(3)

# 
source("plot_trial.R")

plts <- map(1:3, plot_trial, trls_easy, a, d)

#plts[1] + plts[2] + plts[3]

plot(plot_trial(1, trls_hard, a, d))


```