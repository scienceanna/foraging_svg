%  LaTeX support: latex@mdpi.com 
%  For support, please attach all files needed for compiling as well as the log file, and specify your operating system, LaTeX version, and LaTeX editor.

%=================================================================
\documentclass[vision,article,submit,pdftex,moreauthors]{Definitions/mdpi} 
% For posting an early version of this manuscript as a preprint, you may use "preprints" as the journal and change "submit" to "accept". The document class line would be, e.g., \documentclass[preprints,article,accept,moreauthors,pdftex]{mdpi}. This is especially recommended for submission to arXiv, where line numbers should be removed before posting. For preprints.org, the editorial staff will make this change immediately prior to posting.

%----------
% submit
%----------
% The class option "submit" will be changed to "accept" by the Editorial Office when the paper is accepted. This will only make changes to the frontpage (e.g., the logo of the journal will get visible), the headings, and the copyright information. Also, line numbering will be removed. Journal info and pagination for accepted papers will also be assigned by the Editorial Office.


%=================================================================
% MDPI internal commands
\firstpage{1} 
\makeatletter 
\setcounter{page}{\@firstpage} 
\makeatother
\pubvolume{1}
\issuenum{1}
\articlenumber{0}
\pubyear{2022}
\copyrightyear{2022}
%\externaleditor{Academic Editor: Firstname Lastname} % For journal Automation, please change Academic Editor to "Communicated by"
\datereceived{} 
\dateaccepted{} 
\datepublished{} 
%\datecorrected{} % Corrected papers include a "Corrected: XXX" date in the original paper.
%\dateretracted{} % Corrected papers include a "Retracted: XXX" date in the original paper.
\hreflink{https://doi.org/} % If needed use \linebreak
%\doinum{}
%------------------------------------------------------------------
% The following line should be uncommented if the LaTeX file is uploaded to arXiv.org
%\pdfoutput=1

%=================================================================
% Add packages and commands here. The following packages are loaded in our class file: fontenc, inputenc, calc, indentfirst, fancyhdr, graphicx, epstopdf, lastpage, ifthen, lineno, float, amsmath, setspace, enumitem, mathpazo, booktabs, titlesec, etoolbox, tabto, xcolor, soul, multirow, microtype, tikz, totcount, changepage, attrib, upgreek, cleveref, amsthm, hyphenat, natbib, hyperref, footmisc, url, geometry, newfloat, caption

%=================================================================
%% Please use the following mathematics environments: Theorem, Lemma, Corollary, Proposition, Characterization, Property, Problem, Example, ExamplesandDefinitions, Hypothesis, Remark, Definition, Notation, Assumption
%% For proofs, please use the proof environment (the amsthm package is loaded by the MDPI class).

%=================================================================
% Full title of the paper (Capitalized)
\Title{Foraging: The Triumphant Return}

% MDPI internal command: Title for citation in the left column
\TitleCitation{Title}

% Author Orchid ID: enter ID or remove command
\newcommand{\orcidauthorA}{0000-0002-7368-2351} % Add \orcidA{} behind the author's name
\newcommand{\orcidauthorB}{0000-0002-4122-9499} % Add \orcidB{} behind the author's name
\newcommand{\orcidauthorC}{0000-0003-2677-1965}

% Authors, for the paper (add full first names)
\Author{Alasdair D.F. Clarke $^{1}$\orcidA{}, Amelia Hunt $^{2}$\orcidB{} and Anna E. Hughes\orcidC{} $^{1,}$*}

%\longauthorlist{yes}

% MDPI internal command: Authors, for metadata in PDF
\AuthorNames{Firstname Lastname, Firstname Lastname and Firstname Lastname}

% MDPI internal command: Authors, for citation in the left column
\AuthorCitation{Lastname, F.; Lastname, F.; Lastname, F.}
% If this is a Chicago style journal: Lastname, Firstname, Firstname Lastname, and Firstname Lastname.

% Affiliations / Addresses (Add [1] after \address if there is only one affiliation.)
\address{%
$^{1}$ \quad Affiliation 1; e-mail@e-mail.com\\
$^{2}$ \quad Affiliation 2; e-mail@e-mail.com}

% Contact information of the corresponding author
\corres{Correspondence: e-mail@e-mail.com; Tel.: (optional; include country code; if there are multiple corresponding authors, add author initials) +xx-xxxx-xxx-xxxx (F.L.)}

% Current address and/or shared authorship
%\firstnote{Current address: Affiliation 3} 
%\secondnote{These authors contributed equally to this work.}
% The commands \thirdnote{} till \eighthnote{} are available for further notes

%\simplesumm{} % Simple summary

%\conference{} % An extended version of a conference paper

% Abstract (Do not insert blank lines, i.e. \\) 
\abstract{Foraging refers to search involving multiple targets or multiple types of targets, and as a model task has a long history in animal behaviour and human cognition research. Foraging behaviour is usually operationalized using summary statistics, such as average distance covered during target collection (the path length) and the frequency of switching between target types. We recently introduced an alternative approach, which is to model each instance of target selection as random selection without replacement. Our model produces estimates of a set of foraging biases, such as a bias to select closer targets or targets of a particular category. Here we apply this model to predict individual target selection events. We add a new start position bias to the model, and generate foraging paths using the parameters estimated from individual participants’ pre-existing data. The model predicts which target the participant will select next with a range of accuracy from 43\% to 69\% across participants (chance is 11\%). The model therefore explains a substantial proportion of foraging behaviour in this paradigm. The situations where the model makes errors reveal useful information to guide future research on those aspects of foraging that we have not yet explained.}

% Keywords
\keyword{foraging; visual search; bayesian model; decision; strategy.} 

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% The order of the section titles is: Introduction, Materials and Methods, Results, Discussion, Conclusions for these journals: aerospace,algorithms,antibodies,antioxidants,atmosphere,axioms,biomedicines,carbon,crystals,designs,diagnostics,environments,fermentation,fluids,forests,fractalfract,informatics,information,inventions,jfmk,jrfm,lubricants,neonatalscreening,neuroglia,particles,pharmaceutics,polymers,processes,technologies,viruses,vision


\section{Introduction}

Foraging, the act of searching for and gathering multiple targets (such as food), has been studied in both non-human animal contexts \cite{dawkins1971} and in human psychology studies \cite{kristjansson2014}. Foraging engages a wide range of different perceptual, cognitive, and decision-related skills, and is an ecologically relevant behaviour for most species, including humans. For these and other reasons, a sustained interest in foraging has developed our understanding of how we search for multiple instances of multiple types of targets. For example, the classic marginal value theorem \cite {charnov1976} is generally good at predicting when an organism will decide to stop searching in a patch and move on to another, based on the finding rate dropping below an expected rate, and taking into account the relative energy costs of traveling between patches versus staying within a patch. There has also been a concerted effort to characterize the spatial patterns of foraging behaviour. The Lévy Walk, for example, has been argued to be a good description of the foraging path many species take when the locations of targets are unknown (e.g. \citep {bartumeus2005}, but see also \citep{benhamou2007}). 

Another general principle of foraging is that of the “search image”, which describes the perceptual features that a foraging animal can use to identify targets or classes of targets \cite{Dukas1993}. A limited capacity for complex search images means that dividing attention over more than one kind of potential search target can impede search, particularly when what distinguished targets from distractors is not a simple feature like colour or motion. As a result, foragers tend to search in “runs” of one type of target before switching to another. In humans, this behaviour has been studied using computer-based displays of targets and distractors (e.g. \citep{ kristjansson2014}), in which participants must “collect” targets (by clicking or tapping on them) and ignore distractors. When targets can be identified based on a single, easily distinguishable feature (for example, find all the red and green shapes and ignore the blue and yellow shapes) participants tend to switch frequently between different types of targets, taking a relatively efficient path through the array. In contrast, when targets are defined based on a combination of more than one feature (e.g. find all the red squares and green circles and ignore the red circles and green squares), participants tend to switch far less frequently, often selecting all the targets of one category and then all the targets of the other. Consequently, the path taken to collect the targets is less efficient. In other words, frequent switching between target types limits the distance travelled between targets, while collecting all the targets of each type before switching to the other type sacrifices some efficiency of movement in service of reducing the mental workload. 

Different aspects of foraging behaviour, such as the search image and the search path, clearly interact with one another, but most research has tried to understand them separately. In many cases, the pattern of foraging behaviour has also been studied using aggregate measures calculated on the level of a trial, such as the mean number of 'runs' (where the same type of target is selected multiple times in a row) or the total number of targets found during the longest run. However, we have recently developed a new model to analyse foraging data, based on a sampling without replacement procedure \cite{clarke2022foraging}. By using this model, we demonstrated that we were able to break down foraging into a number of different cognitive biases, such as a preference to stick to the same target type, or a preference for nearby targets, and used this model to successfully re-analyse data from a number of open access datasets \cite{kristjansson2014, thornton2020, clarke2022, tagu_kristjansson_2021}. 

While our model is able to do a good job of allowing us to understand biases at the level of a trial (e.g. conjunction vs. feature search differences \cite{kristjansson2014} or the influence of high value targets compared to low value targets \cite{tagu2020}), we did not originally investigate to what extent our model was able to predict target-by-target behaviour in any given trial. However, as the model is based on target-by-target level information, it is possible for us to use this information to ask: to what extent does our model predict foraging behaviour within a trial? Can it predict exactly which target a participant will pick next? And if the model makes mistakes, can we understand where it is failing, in order to improve our understanding of how human foraging behaviour operates?

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Materials and Methods}

\subsection{Datasets}

We will a previously published dataset to explore how well our model can account for trial-level behaviour. We will start with the visual foraging data from \cite{clarke2022}. This is an attractive dataset for our needs as it is very close to the ``classic'' visual foraging paradigm \citep{kristjansson2014}, yet with a substantially larger sample of participants. (The key difference between paradigms is that \cite{kristjansson2014} used an ipad and finger foraging, while \cite{clarke2022} used a desktop computer and mouse clicks). Furthermore, we have already demonstrated that our model can capture individual differences in foraging behaviour \citep{clarke2022}. As such, we will only give a brief overview of the data here and refer the reader to these earlier papers for a more details.

To test how well our methods generalise, we use the dataset from \citep{kristjansson2014}. We split this into a training dataset (50\% of the original data i.e. 10 trials per participant) and a test dataset (the remaining 50\% of the data). The foraging model is fit on the training dataset, and the parameters from this model are then used to predict behaviour at the level of individual trials. We show that our methods appear to generalise beyond \citep{clarke2022}: further details can be found in the \textit{Supplementary Materials}.


\subsection{A Model for Visual Foraging}

We will make use of the foraging model from \cite{clarke2022}. This treats foraging as a sampling without replacement process in which each item $i$ has probability $p_i$ of being selected as the next target. The $p_i$ depend on four parameters:

\begin{itemize}
    \item $b_A$ - preference for selecting items of type $A$ rather than $B$.
    \item $b_S$ - preference for selecting items of the same type as the previously selected item.
    \item $\sigma_d$ - preference for selecting items close to the previously selected item.
    \item $\sigma_{\theta}$ - preference to keep selecting items along a straight line versus changing direction.
\end{itemize}

These combine to give:

\begin{linenomath}
\begin{equation}
    w_i = g\left(b_at_i + b_sm(t_i, t_{i-1})\right) \times \rho_d(i, i-1) \rho_{\theta}(i, i-1)
\end{equation}
\end{linenomath}

where $t_i = 1$ if item $i$ is of class $A$ and 0 otherwise, $m(t_i, t_{i-1}) =1$ if item $i$ is the same class as the previously selected item. $\rho_d$ and $\rho_{\theta}$ measure proximity and the effect of direction:

\begin{linenomath}
\begin{equation}
    \rho_d = e^{-\sigma_dd(i,j)}
\end{equation}
\end{linenomath}

\begin{linenomath}
\begin{equation}
    \rho_{\theta} = e^{-\sigma_d\theta(i,j)}
\end{equation}
\end{linenomath}

$d(i,j)$ is simply the Euclidean distance between items $i$ and $j$, while $\theta$ is defined as:

\begin{linenomath}
\begin{equation}
    \theta(i,j) = \frac{f(\text{atan2}(i, j) - \text{atan2}(i-1, i))}{\pi}
\end{equation}
\end{linenomath}

with $f(\phi_1, \phi_2) = \text{min}((\phi_1 - \phi_2) \% 2\pi, (\phi_2 - \phi_1) \% 2\pi)$. This model is implemented in a multi-level framework, allowing each of the four parameters to vary from participant to participant. Further details including priors and full code can be found with \cite{clarke2022foraging}.

Note: while our model returns estimates of full posterior probability distribution for each parameter, to reduce computational complexity and make it easier to compare to the empirical data, we will work with the means of these distributions for our parameter values. 

\subsection{Software environment}
 
 We used R, Stan, tidyverse version etc. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results: Evaluating the Model}

\subsection{Trial-level predictions}

To assess how well our model can account for behaviour at the level of individual trials, we start by stepping through each trial in the data from \cite{clarke2022}. For each target selection, we can estimate how likely each of the remaining items are to be selected using the parameters from our posterior model. As can be seen in Figure \ref{fig:cal}, the model is putting somewhere between 25\% and 75\% of the weight on the target that is selected next, easily outperforming a chance $1/n$ baseline. We can also see  that the model is well calibrated in that the probabilities assigned to the largest target manage to capture how well often that target is actually selected. However, we can see a decrease in accuracy when it comes to how often ``runner-up`` candidate items are selected as our model appears to systematically undervalue these (presumably by putting too much of the probabilistic weight on low chance items).

\begin{figure}[H]
\centering
\includegraphics[width=12 cm]{Figures/qjep_preds.pdf}
\caption{(\textit{left}) Posterior probabilities for target selections during a visual foraging task. Each dot shows the  data from an individual participant, averaged over trials, in a condition and the shaded region indicates the interval in which we expect 67\% of participants to fall. The dashed line indicates chance performance. (\textit{centre})  Calibration plot for our foraging model. The $x$-axis gives the largest weight assigned by the model while the $y$-axis shows how often that target was actually selected by a human participant. (\textit{right}) This plot shows how often the 2nd and 3rd ranked items are selected based on the weights assigned by the model.}
\label{fig:cal}
\end{figure}   

Figure \ref{fig:cal2} hints at some individual differences when it comes to how predictable the model is: we can see a lot of variation in the size of the weights. However, interestingly, we find that this variability can be explained by our model parameters: nearly all of the differences between people, and between feature and conjunction conditions, can be explained by the $bP$ parameter, which is our proximity bias. The model is doing a better job with participants who show a stronger proximity bias (see \textit{Supplementary Materials} for more details).

\begin{figure}[H]
\centering
\includegraphics[width=12 cm]{Figures/qjep_indiv_diff.pdf}
\caption{(\textit{left}) Prediction scores for participants. Boxplots show quartile range and the grey lines indicate individual participants. The dots indicate outliers. (\textit{right}) Accuracy of our model varies with the strength of an individual’s \textit{bP} parameter. We can see two clear outlier participants (marked with an X).}
\label{fig:cal2}
\end{figure}   

\begin{figure}[H]
\centering
\includegraphics[width=12 cm]{Figures/qjep_ex_paths.pdf}
\caption{(\textit{top}) Two randomly selected trials in which the model does a good job in accounting for human behaviour. Red arrows indicate places where the model prediction deviates from participant behaviour. When participants diverge from the model's prediction, it appears to be due to some form of path-length optimisation. (\textit{bottom}) Two randomly selected examples in which the model does a less good job in accounting for the order in which human participants selected the items.}
\label{fig:qjep_paths}
\end{figure} 

\section{Improving the model: location of the first target selection}

One weakness of the foraging model by \cite{clarke2022foraging} is that it makes little attempt to predict which item will be selected first. In this section we aim to improve this by modelling participant bias (or preference) in their choice of initial target selection. Our approach is inspired by previous work on the central bias in scene viewing \cite{clarke_tatler2014, clarke2017}. These studies characterised fit truncated Gaussian distributions to the  $(x,y)$ coordinates of fixation locations. Here we will do similar, and fit beta distributions\footnote{We use beta distributions here as ADFC couldn't work out the syntax for specifying a mixture model using truncated Gaussians in Stan. But, in hindsight, beta distributions are probably a better choice anyway!} to locations of the initial target selections in the visual foraging task. 

Figure \ref{fig:qjep_init_sel_hex} shows the distribution of the locations of first target selections. We can see that unlike the central bias in fixations during scene viewing, the distribution of initial target locations appear to be bimodal: while most targets are positioned in the top left hand corner, there is a second smaller distribution of central target selections. This appears to be due to different participants choosing to utilise different strategies rather than within-subject variation. However, we have no evidence that these different initial strategies affect the model parameters in any systematic way (see \textit{Supplementary Material}). We try two different modelling approaches (multi-level and mixture) to assess whether the less complex mixture model is able to account for the patterns of starting positions seen.

\begin{figure}[H]
\centering
\includegraphics[width=12 cm]{Figures/init_sel_hex_plot.pdf}
\caption{Hexagonal heatmaps showing the two-dimensional distribution of the location of initial target selections. Grey areas indicate cells with a count of 0. (\textit{left}) shows the distribution over all initial target selections while (\textit{right}) shows the distribution of each participant's median initial selection. }
\label{fig:qjep_init_sel_hex}
\end{figure} 

\subsection{Multi-level Modelling Results}

We first consider a multi-level model in which we fit beta distributions to the $x$ and $y$ coordinates of each participant's initial target selections\footnote{Full details of the analysis can be found in the \textit{Supplementary Materials}.}:

\begin{equation}
    x_i \sim Beta(a_{x,i}, b_{x,i})
    \label{eq:beta1x}
\end{equation}
\begin{equation}
    y_i \sim Beta(a_{y,i}, b_{y,i})
    \label{eq:beta1y}
\end{equation}

The results are shown in Figure \ref{fig:qjep_init_sel_mdl} and we can clearly see that most participants either have a strong bias to $x=0$ and $y=0$, or a more diffuse central bias. 

\begin{figure}[H]
\centering
\includegraphics[width=12 cm]{Figures/init_sel_mdl.pdf}
\caption{Posterior fits for (\textit{left}) horizontal and (\textit{middle}) vertical locations of the initial target selection. Each line represents a different participant. (\textit{right}) A scatter plot showing the $x$ and $y$ posterior means for each participant.}
\label{fig:qjep_init_sel_mdl}
\end{figure} 

\subsection{Mixture Modelling}

While the multi-level model outlined above appears to offer a good fit for the between-subject differences in initial target selection location, it comes at the expense of requiring four parameters per participant (= 290 for the 58 participants in our dataset). Given that the majority of our participants appear to be following one of two distinct strategies, we can potentially simplify our model by using a two-component mixture model:

\begin{equation}
    x_i \sim \lambda_i \times Beta(a_{x,1}, b_{x,1}) + (1-\lambda_i) \times Beta(a_{x,2}, b_{x,2})
    \label{eq:beta2x}
\end{equation}
\begin{equation}
    y_i \sim \lambda_i \times Beta(a_{y,1}, b_{y,1}) + (1-\lambda_i) \times Beta(a_{y,2}, b_{y,2})
    \label{eq:beta2y}
\end{equation}

This reduces the number of parameters to eight (to specify the two Beta distributions) and then one $\lambda$ value per participant (= 66 in our dataset). While the formulae in Equations \ref{eq:beta2x} and \ref{eq:beta2y} appear more complex than those in Equations \ref{eq:beta1x} and \ref{eq:beta1y}, the model requires less than a quarter as many free parameters! 

The model fit is shown in Figure \ref{fig:qjep_init2_sel_mdl}. We can see that the two components identified by the model clearly correspond to the top corner and diffuse-central strategies discussed above. We can also see from the lambda values that our participants take on a range of different mixtures between the two. 

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{Figures/init_sel2_mdl.pdf}
\caption{Posterior fits for the (\textit{left}) horizontal and (\textit{middle}) vertical locations of the initial target selection. (\textit{right}) shows the mixing parameters for each participant, with the error bar indicating the $95\%$ HDPI.}
\label{fig:qjep_init2_sel_mdl}
\end{figure} 

\subsection{Posterior Predictions}

To compare the two approaches, we assess how well they fit the empirical data: we can use the fitted distributions to calculate the weight assigned to the selected target by each method for each trial in our dataset (Figure \ref{fig:qjep_init_sel_pred}). We can see that both models give a similar distribution of target weights and accuracies. Overall, both methods  select the correct target in around half of the trials. Interestingly, as above, there is considerable variation between participants, with participants who favour the top-left corner being easier to predict. 

\begin{figure}[H]
\centering
\includegraphics[width=12 cm]{Figures/qjep_init_sel_preds.pdf}
\caption{(\textit{Top}): a count of initial weight values predicted from both methods (mixture and multi-level). (\textit{Bottom left}): the proportion of times the first selection was correct for both methods.  (\textit{Bottom left}): as lambda (the model parameter underlying initial weight) increases, accuracy increases i.e. the model is more accurate for the cases where participants are selecting a corner as their initial target selection.} 
\label{fig:qjep_init_sel_pred}
\end{figure} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion}

In our original model of visual foraging \citep{clarke2022foraging}, we were able to robustly measure aggregate parameters that underlie behaviour, at both an overall mean level across an experiment, and at the level of individual participants. In the current manuscript, we asked a more difficult question: could this model make target-by-target predictions, guessing which target would be picked next on any given trial? Despite this being a relatively difficult task, we found that our model was in some cases over 80\% accurate, demonstrating that at least in some cases, participants' behaviour in this task is remarkably predictable. 

A benefit of our approach is that it is relatively easy to interrogate the model to try to understand why prediction accuracy is higher for some participants and trials than others. The model is generally better at feature search compared to conjunction search, and also seems to be better at predicting participants with a stronger proximity bias. The model also seems to assign more weight to 'runner-up' items than real humans do, perhaps suggesting a 'peakier' choice distribution for real foragers, with only a few targets seriously considered, and everything else being approximately zero weighted. 
 
However, we can also delve deeper into exploratory hypotheses about other factors that might influence predictability. One suggestion is that the people who our model is able to predict are in some way the most 'optimal'. If this is the case, we might expect that model predictability should correlate with how good people are at the task, although we do not find any evidence that model accuracy correlates with reaction time on a by-participant or by-trial basis (see \textit{Supplementary Materials}). This may be due to the fact that this task was not time-limited, and instead participants were encouraged to be as accurate as possible (in fact, the trial restarted if the participant clicked on a non-target \citep{clarke2022}). However, there is some evidence that on a target-by-target basis, faster moves from one target to the next are more easily predicted by the model (see \textit{Supplementary Materials}). This likely is linked to the fact that our model is better at predicting participants with a stronger proximity bias, as targets that are closer to the previous target are also likely to be selected more quickly. Intuitively, it makes sense that the model struggles to make predictions when a participant has exhausted a local patch and may need to make a bigger jump to a more distant area to carry on collecting targets, as there may be multiple candidates and distance may no longer be such a good predictor (for example, people may choose to move to the next most densely populated patch). 
 
There has been some previous work on within-trial behaviour in the context of foraging. For example, \citep{tagu2020} found that inter-target times vary across a trial (with targets selected later in the trial being slower) and inter-target distances also vary (with targets selected later in the trial having greater inter-target distances). Similarly, \citep{thornton2022foraging} showed that switches between target categories can be characterised by a change in foraging 'tempo', and if participants are forced to forage at a particular speed, by asking them to synchronise with an auditory signal, higher tempos led to a systematic decrease in the probability of participants switching between target categories. In the current version of the model, we do not consider time explicitly (although distance is likely to be highly correlated with time in this task). However, it would be possible to extend the model to incorporate timing information, and this may help to improve the target-by-target predictions.
 
From inspection of Figure \ref{fig:qjep_paths}, we noticed that in some cases, participants may be using a type of local path minimisation procedure, selecting a further away target in order to minimise overall path length, which the model does not always seem to predict. Could this help to explain some of the mistakes the model makes? Overall, the model doesn't seem to make substantially longer paths through the items than the human participants (see \textit{Supplementary Materials}). However, there is a relationship between model accuracy and the proportion of times that participants select paths shorter than the model: as model accuracy increases, people seem to be more likely to have paths shorter than the model predicts. This seems to suggest that path minimisation is not a good explanation for the targets our model is failing to capture, and instead the people that are difficult to predict are taking longer paths than the model thinks they should take.

Our exploratory analyses seem to hint at the idea that the people who are difficult for the model to predict are those who do not behave in a manner that optimises path length. It could be that these participants are simply unpredictable, and do not behave in a consistent manner on a target-by-target basis, making it difficult for a model to predict their behaviour at this level of granularity. However, it is possible that there are other factors that we have not taken into account that could further help explain these discrepancies. For example, it is possible that relatively unpredictable participants use multiple rules and switch between them in ways that we are not currently capturing in the model. Some participants may also lose track of where they are more frequently (perhaps due to attentional lapses) and thus may show foraging patterns that are predictable apart from unpredictable discontinuities, which again may not be easy to model. Finally, the displays had artificial discontinuities (i.e. screen edges) which may lead to different behaviour in different participants: some may carry out the task as if they are reading, reaching the right hand edge and then swinging back to the left, whereas others may 'bounce' off an edge they have just reached. Our model may well be able to predict the latter, but could struggle with the former behaviour.

To a large extent, initial item selection strategy seems to be stable within a participant, with people either starting in the corner or the centre of the screen. Our model is therefore also relatively accurate for selecting the initial target, selecting the correct target in around half of the trials (with a mixture model performing similarly to a multi-level model with many more parameters). Participants who prefer the top left corner were easiest to predict, probably because these participants formed the majority group in this experiment. Our results are congruent with \citep{kristjansson2022moving}, who found a bottom left bias in a 3D environment They suggest this may be an advantageous strategy because it allows your foraging to be more organised (e.g. in an 'S-shape', as has been found in real world search tasks \citep{riggs2017importance}). The smaller group of participants who prefer to start in the centre of the screen may be displaying similar behaviour to the well-known central bias in eye movements \citep{tatler2007central, tseng2009quantifying, clarke2014deriving} However, it is worth noting that these findings probably depend strongly on the context of the task. For example, there was no obvious central bias subgroup in \citep{kristjansson2014}, possibly reflecting the fact that this experiment was carried out on an iPad, compared to the computer screen and mouse used in \citep{clarke2022}. One challenge for future modelling work is to what extent we should try to account for these types of task-specific details: by incorporating them, we can generate better predictions for the experiment we are currently modelling, but perhaps at the cost of generalisability to other paradigms.

One of the key benefits of computational models is that they allow us to rigorously test how well we are able to predict behaviour, and can provide insights into what factors we are not capturing that may have important influences on how participants complete a task. We have demonstrated that our foraging model \cite{clarke2022foraging} is not only able to predict behaviour on aggregate, but can also make reasonable predictions at the target-by-target level. It is particularly good if a participant has a strong proximity bias, and on 'feature' trials where participants are normally switching between target types fairly frequently. It is also good at predicting where participants will start on a trial. The model finds it more difficult to predict 'discontinuous' jumps, which could be caused by a range of factors: a more difficult foraging task (e.g. conjunction searches), local path length minimisation, inattention, or aspects of the physical search space (e.g. edges of the screen). However, it would be possible to extend the model to incorporate these factors e.g. by incorporating a heuristic that would allow for local path minimisation, or introducing time as a predictor in the model. Our findings also suggest possible future directions for empirical work, such as evaluating the effect of inattention on foraging behaviour, or how the foraging targets are organised in space. We suggest that computational modelling is a powerful tool for helping us to understand behaviour, both by incorporating previous research into a shared framework, and by making testable predictions for future work.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{6pt} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% optional
%\supplementary{The following are available online at \linksupplementary{s1}, Figure S1: title, Table S1: title, Video S1: title.}

% Only for the journal Methods and Protocols:
% If you wish to submit a video article, please do so with any other supplementary material.
% \supplementary{The following are available at \linksupplementary{s1}, Figure S1: title, Table S1: title, Video S1: title. A supporting video article is available at doi: link.} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\authorcontributions{Conceptualization, A.D.F.C., A.H. and A.E.H.; methodology, A.D.F.C.; software, A.D.F.C.; formal analysis, A.D.F.C.; data curation, A.D.F.C., and A.E.H.; writing---original draft preparation, A.D.F.C.; writing---review and editing, A.H. and A.E.H.; visualization, A.D.F.C. and A.E.H.; funding acquisition, A.D.F.C. and A.H. All authors have read and agreed to the published version of the manuscript.}

\funding{This research was funded by NAME OF FUNDER grant number XXX.}

\institutionalreview{Not applicable (secondary data analysis only).}

\informedconsent{Not applicable (secondary data analysis only).}

\dataavailability{Data supporting reported results can be found on Github at XXX. The datasets analysed in this study can be found at \url{https://osf.io/y6qbv/} and \url{https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0100752}.} 

\acknowledgments{In this section you can acknowledge any support given which is not covered by the author contribution or funding sections. This may include administrative and technical support, or donations in kind (e.g., materials used for experiments).}

\conflictsofinterest{The authors declare no conflict of interest.} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{adjustwidth}{-\extralength}{0cm}
%\printendnotes[custom] % Un-comment to print a list of endnotes

\reftitle{References}

%=====================================
% References, variant A: external bibliography
%=====================================
\bibliography{literature.bib}

\end{adjustwidth}
\end{document}

