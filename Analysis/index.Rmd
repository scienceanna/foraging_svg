---
title: 'SVG: The Paper'
author: "Alasdair Clarke, Amelia Hunt &  Anna Hughes"
date: "13/04/2022"
output:
  tufte::tufte_html:
    number_sections: true
    fig_height: 4
    bibliography: literature.bib
    link-citations: yes  
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
options(digits = 3)
set.seed(2022)
```


```{r, message=FALSE}
# attach packages
library(tidyverse)
library(patchwork)
library(tidybayes)
library(ggrepel)
library(truncnorm)
library(modelr)
library(brms)
```

```{r}
# set ggplot2 options
theme_set(ggthemes::theme_tufte() + 
            theme(plot.background = element_rect(fill = "#fffff8", colour = "#fffff8")))

# some more colours
col3 = c("#d04e00", "#ddc000","#34b6c6",  "#79ad41", "#4063a3")
col2 = c("#d04e00", "#34b6c6")
                
our_cols <- c("#1b9e77", "#7570b3")
options(ggplot2.discrete.fill = our_cols ,
        ggplot2.discrete.colour = our_cols)
```

Terminology questions:
- item v target
- participant v observer

# Trial Level Posterior Predictions{#label}

```{r echo = FALSE}
source("functions/get_model_params.R")
source("functions/compute_weights.R")
```

We will use the visual foraging data from @clarke2022 to assess the model's trial level performance. As the current version of our model only takes the $p_A$ bias into account for the initial target selection, we will ignore these data points for now. 

```{r, echo = TRUE}
# read in data
d <- read_csv("data/clarke_2020_qjep.csv", show_col_types = FALSE) %>%
  mutate(condition = as_factor(condition),
         condition = fct_recode(condition, feature = "1", conjunction = "2"),
         targ_type = as_factor(targ_type))
```

We will use the model fit as detailed in @clarke2020compbio. For simplicity sake, we will characterise all of our estimated posterior probability densities with just the mean value. This gives us four parameters per participant: $b_S, b_A, b_P, b_D$. 

Using these fitted values, we can now step through each target selection in the dataset and computer the probability weights our model assigns to each remaining target. For each target selection, we record:

- the item assigned the highest weight by our model.
- whether this item was then selected by the human participant. 


```{r echo = TRUE}
# item weights pre-computed - see scripts/extract_item_weights_using_model.R
a <- readRDS("scratch/qjep_model_weights.rda") %>%
  filter(found != 1) # remove initial selections

# compute the average weight for each participant x condition x item selection
# also compute the proportion of times in which the item selected by the 
# participant was judged the most likely to be selected by our model.
a %>% group_by(condition, observer, found) %>%
  summarise(meanb = mean(b),
            prop_best = mean(selected_max), .groups = "drop") %>%
  # finally, also compute what we would expect under a null-model
  mutate(chance = 1/(41-found))-> a_agg
```

We can then plot this data to show how our ability to predict which target will be selected next varies throughout a trial. We can see a difference between *feature* and *conjunction* foraging, with *feature* foraging being more predictable. 

```{r, fig.cap = "(*left*) The average weight assigned to each selected target by our model. (*right*) The proportion of trials the item with the largest assigned weight was selected by the participant.  Each dot shows data from an individual participant in a condition and the shaded region indicates the interval in which we expect 67% of participants to fall."}

# plot target selected weights
ggplot(a_agg, aes(x = found, y = meanb, colour = condition, fill = condition)) + 
  geom_jitter(data = filter(a_agg, found<40), width = 0.1, height = 0, alpha = 0.2) + 
  stat_lineribbon(.width = 0.67, alpha = 0.50) +
  geom_path(data = filter(a_agg, observer == 1, condition == "feature"), 
                          aes(y = chance), linetype = 2, colour = "black") + 
  geom_point(data = tibble(x=40, y=1), aes(x, y), size = 1.5, colour = "black", fill = "grey") + 
  scale_x_continuous(breaks = c(2, 10, 20, 40), "target selection", expand = c(0.01, 0.01)) + 
  scale_y_continuous("average weight from model", expand = c(0.01, 0.01)) -> plt_b

ggplot(a_agg, aes(x = found, y = prop_best, colour = condition, fill = condition)) + 
  geom_jitter(data = filter(a_agg, found<40), width = 0.1, height = 0.00, alpha = 0.2) + 
  stat_lineribbon(.width = 0.67, alpha = 0.50) +
  geom_path(data = filter(a_agg, observer == 1, condition == "feature"), 
            aes(y = chance), linetype = 2, colour = "black") + 
  geom_point(data = tibble(x=40, y=1), aes(x, y), size = 1.5, colour = "black", fill = "grey") + 
  scale_x_continuous(breaks = c(2, 10, 20, 40), "target selection", expand = c(0.01, 0.01)) + 
  scale_y_continuous("proportion most likely was selected", expand = c(0.01, 0.01)) -> plt_c

plt_b + plt_c + plot_layout(guides = "collect")  &
  theme(legend.position = 'bottom',
        legend.direction = 'horizontal')

rm(plt_c)

```


## Calibration

Is our model calibrated? By this we mean, if our model assigns an item a probability $p$ of being selected next, then is it actually selected (by the human participant) this often?

We will calculate this as follows: for each target selection (on each trial, for each participant) we look at the weight assigned to the most likely item, and then look at whether it was selected or not. 


```{r, echo = TRUE}
my_breaks <- seq(0.0, 1, 0.05)

a %>% mutate(b_bin = cut(max_b, breaks = my_breaks, labels = FALSE)) %>%
  group_by(condition, b_bin) %>% 
  summarise(acc = mean(selected_max), .groups = "drop") %>%
  mutate(b_bin = as.numeric(b_bin)/(length(my_breaks)-1)) -> a_cut
```

We can see from the Figure below that our model is well-calibrated (at least in terms of the target with the highest assigned weight). If we repeat this calculation for the 2nd and 3rd ranked candidate items we can see that we slightly underweight these items, but we are still pretty accurate. 

```{r, fig.height=3.5, fig.cap = "(*left*) Calibration plot for our foraging model. The *x*-axis gives the largest weight assigned by the model while the *y*-axis shows how often that target was actually selected by a human participant. (*right*) This plot shows how often the 2nd and 3rd ranked items are selected based on the weights assigned by the model."}

ggplot(a_cut, aes(b_bin, acc, colour = condition, fill = condition, shape = condition)) + 
  geom_point() + 
  geom_line(stat = "smooth", method = "loess", 
            formula = y ~ x, alpha = 0.65, size = 2, se = F) +
  geom_abline(linetype = 2) +
  scale_x_continuous("largest item weight") +
  scale_y_continuous("proportion of times selected") +
  theme(legend.position = "bottom") -> plt1

a %>% select(max_b2, max_b3, selected_max2, selected_max3) %>%
  unite(`2`, max_b2, selected_max2) %>%
  unite(`3`, max_b3, selected_max3) %>%
  pivot_longer(c(`2`, `3`), names_to = "rank", values_to = "b") %>%
  separate(b, into = c("b", "selected"), sep = "_", convert = TRUE) %>%
  mutate(b_bin = cut(b, breaks = my_breaks, labels = FALSE)) %>%
  group_by(b_bin) %>% 
  summarise(acc = mean(selected), .groups = "drop") %>%
   mutate(b_bin = as.numeric(b_bin)/(length(my_breaks)-1)) %>%
  filter(is.finite(acc)) %>%
  ggplot(aes(b_bin, acc)) + 
  geom_point() + 
  geom_line(stat = "smooth", method = "loess", 
            formula = y ~ x, alpha = 0.65, size = 2, se = F) +
  geom_abline(linetype = 2) +
  scale_x_continuous("runner up item weights") +
  scale_y_continuous("proportion of times selected") -> plt2

 plt1 + plt2

ggsave("../Figures/qjep_preds.pdf", 
       plt_b + plt1 + plt2 + 
         plot_layout(guides = "collect") & theme(legend.position = "bottom"),
       width = 10, height = 4)


rm(plt1, plt2, plt_a)
```

```{r}
rm(a_cut, my_breaks, plt1, plt2)
```

### Individual Differences

How often does each participant select the target with the largest weight? 

```{r, fig.height=3.5, fig.cap = "Prediction scores for participants. Boxplots show quartile range and the grey lines indicate individual participants. (The dots indicate outliers.)", echo= TRUE}

a_agg %>% group_by(observer, condition) %>%
  summarise(accuracy = mean(prop_best), .groups = "drop") -> a_acc

ggplot(a_acc, aes(x= condition, y = accuracy, fill = condition)) + 
  geom_boxplot() +
  geom_line(aes(group = observer), alpha = 0.25, colour = "sienna4") +
  scale_y_continuous("model accuracy",  
                     limits = c(0.35, 0.8), breaks = seq(0.3, 0.8, 0.1)) -> plt_a

plt_a
```

Is this explained by differences in proximity weighting? i.e., are participants with weaker proximity biases harder to predict? Yes! 

```{r fig.height = 6, fig.cap = "Accuracy of our model varies with the strength of an individual's model parameters. We can see two clear outlier participants (marked with an X). "}
# computed in xxxx.R
fit <- readRDS("scratch/qjep_model_fit.rda") %>%
  full_join(a_acc, by = c("observer", "condition")) %>%
  mutate(is_outlier = if_else(observer %in% c(35, 45), TRUE, FALSE))


fit_plt <- pivot_longer(fit, c(pA, pS, bP, bM), 
                     names_to = "parameter", values_to = "value") 

ggplot(fit_plt, aes(value, accuracy, colour = condition)) + 
  geom_point(aes(shape = is_outlier)) +
  geom_smooth(data = filter(fit_plt, is_outlier == 0), method = "lm", formula = y ~ x, se = FALSE) + 
  scale_shape_manual(values = c(19, 4), guide = "none") + 
  facet_wrap(~parameter, scales = "free") +
  theme(legend.position = "bottom")

plt_b <- ggplot(filter(fit_plt, parameter == "bP"), 
                aes(value, accuracy, colour = condition)) + 
  geom_point(aes(shape = is_outlier)) +
  geom_smooth(data = filter(fit_plt, is_outlier == 0, parameter == "bP"), method = "lm", formula = y ~ x, se = FALSE) + 
  scale_shape_manual(values = c(19, 4), guide = "none") + 
  theme(legend.position = "none") + 
   scale_y_continuous("model accuracy",  
                     limits = c(0.35, 0.8), breaks = seq(0.3, 0.8, 0.1))

ggsave("../Figures/qjep_indiv_diff.pdf", 
       plt_a + plt_b + 
         plot_layout(guides = "collect") & theme(legend.position = "right"), width = 8, height = 4)
```

As the two outlier participants appear to be doing something different, not captured especially well by our model, we will remove them from the rest of this analysis.

After removing them, we can see that individual differences in proximity tuning account for nearly all of the differences in predictability from person to person, and also between the *feature* and *conjunction* conditions.

```{r, echo = TRUE}
dm <- filter(fit, is_outlier == 0)

summary(lm(accuracy ~ condition * bP, 
           data = dm))
```

We can see that the difference in the proximity parameter between conditions appears to account for most of the variables in our model's accuracy from one participant to the next. 

We can also see if the other parameters have an effect. 


```{r echo=TRUE}
summary(lm(accuracy ~ pA + pS + bM + bP, 
           data = dm))
```

Both $p_A$ and $b_M$ also have an effect on our model's accuracy, but note there is only a small increases in $R^2$ from just using $b_P$.

## The relationship between time and model accuracy

### Faster people are more predictable

Is this worth doing?

### Shorter trials are more predictable

```{r fig.cap="The relationship between the time taken to complete a trial and how predictible it is. Dots show empirical data, and the shaded ribbon shows the fixed-effects of a multi-level model. The lines give posterior samples from each indivdiaul observer."}
a %>% select(observer, condition, trial, found, selected_max) %>%
  full_join(d, by = c("observer", "condition", "trial", "found")) %>%
  group_by(observer, condition, trial) %>%
  summarise(time_taken = max(RT),
            acc = mean(selected_max, na.rm = T), 
            .groups = "drop") %>%
  filter(time_taken < 60) %>%
  mutate(time_taken_s = as.numeric(scale(log(time_taken)))) -> timedat

write_csv(timedat, "scratch/time_per_trial.csv")

m <- readRDS("scratch/time_model_acc_model.rds")

fixef(m) %>% knitr::kable()

```




```{r fig.cap="The relationship between the time taken to complete a trial and how predictible it is. Dots show empirical data, and the shaded ribbon shows the fixed-effects of a multi-level model. The lines give posterior samples from each indivdiaul observer."}
timedat %>% data_grid(condition, time_taken_s = seq_range(time_taken_s, 5)) %>%
  add_epred_draws(m, re_formula = NA, ndraws = 100) %>%
  ggplot(aes(time_taken_s, fill = condition)) + 
  stat_lineribbon(aes(y = .epred), alpha = 0.5, .width = c(0.53, 0.97)) +
  geom_jitter(data = timedat, aes(y = acc, colour = condition), 
              alpha = 0.05, width = 0, height = 0.005, colour = "black") +
  scale_x_continuous("scaled log response time (log seconds)") + 
  scale_y_continuous("model accuracy")  + 
  scale_colour_discrete(guide = "none") +
  coord_cartesian(ylim = c(0.4, 0.9)) +
  guides(fill = guide_legend(reverse = TRUE)) +
  facet_wrap(~ condition) -> plt

timedat %>% data_grid(condition, time_taken_s = seq_range(time_taken_s, 5), observer) %>%
  add_epred_draws(m, re_formula = NULL, ndraws = 1) -> d_pred


plt + geom_path(data  = d_pred,
                aes(y = .epred, 
                    group = interaction(condition, observer)),
                alpha = 0.25)

```

```{r}

# 
# my_model <- brm(acc ~ time_taken:condition + (1|observer), timedat)

a %>% select(observer, condition, trial, found, selected_max) %>%
  full_join(d, by = c("observer", "condition", "trial", "found")) %>%
  arrange(observer, condition, trial, found) %>%
  mutate(
    RTd = RT - lag(RT),
    RTd = if_else(found == 1, 0, RTd)) %>%
  filter(RTd < 2, is.finite(RTd)) %>%
  mutate(
    time_bin = cut(RTd,breaks = 50, labels = FALSE)) %>%
  group_by(observer, condition, time_bin) %>%
  summarise(acc = mean(selected_max)) %>%
  ggplot(aes(time_bin, acc, group = observer)) +
  geom_path(alpha = 0.2) +
  geom_smooth(se= F, alpha = 0.1, method = lm) + 
  facet_grid(~condition)

```


```{r}
rm(dm)
```

## Examples of Trials

Let's score every trial by how well the model captures behaviour, and then plot some different examples.

```{r, echo = TRUE}
a %>% filter(!(observer %in% c(35, 45))) %>%
  group_by(observer, condition, trial) %>% 
  summarise(prop_max = mean(selected_max), .groups = "drop") %>% 
  arrange(desc(prop_max)) %>%
  ungroup() -> a_trl
```

```{r fig.height=3.5, fig.cap = "Histogram of how predictiable individual trials are, defined as how often the model assigned the largest weight to the target that was then selected."}
ggplot(a_trl, aes(prop_max)) + 
  geom_histogram(fill = "#d04e00", breaks = seq(0, 1, 0.025), alpha = 0.5) +
  scale_x_continuous("trial predictability")
```

Let's plot some examples oh highly predictable (top 5%), typical (middle 10%) and unpredictable (bottom 5%) trials.

```{r, echo = TRUE}
qs <- quantile(a_trl$prop_max, c(0.05, 0.40, 0.60, 0.95))

trls_hard <- filter(a_trl, prop_max < qs[1]) %>% sample_n(3)
trls_mid  <- filter(a_trl, prop_max > qs[2], prop_max < qs[3]) %>% sample_n(3)
trls_easy <- filter(a_trl, prop_max > qs[4]) %>% sample_n(3)
```

We can see that for these highly predictable trials, we make very few mistakes, and those that we do make appear to be related to some sort of shortest-path optimization on the part of the participant. 

```{r fig.cap = "A random selection of *good* trials. When participants diverge from the model's prediction, it appears to be due to some form of path-length optimisation."}

source("functions/plot_trial.R")
plts1 <- map(1:3, plot_trial, trls_easy, a, d)

plts1[[1]] + plts1[[2]] +
  plot_layout(nrow = 1, widths = c(1,1)) &
   theme(plot.margin = unit(c(.2,.2,.2,.2), "cm")) &
   theme(plot.background = element_rect(color  = "#fffff8", size = 2,linetype = 'dotted', fill ="#fffff8")) &
   theme(text=element_text(size=14,  family="serif"))
```

Here are some typical trials. Possible we have the path-length minimization issue again. 

```{r fig.cap = "A random selection of *typical* trials. Along with path-minimising behaviour, we can also see some disagreement around switching item type."}
plts2 <- map(1:3, plot_trial, trls_mid, a, d)

plts2[[1]] + plts2[[2]] +
  plot_layout(nrow = 1, widths = c(1,1)) &
   theme(plot.margin = unit(c(.2,.2,.2,.2), "cm")) &
   theme(plot.background = element_rect(color  = "#fffff8", size = 2,linetype = 'dotted', fill ="#fffff8")) &
   theme(text=element_text(size=14,  family="serif"))
```

Now let's look at less predictable (bottom 5%-tile) trials. Even here, the model does a relatively good job. 

```{r fig.cap = "A random selection of *hard* trials. It is harder to make out much of a pattern here, but we can see that the human participants jump around more."}
plts3 <- map(1:3, plot_trial, trls_hard, a, d)

plts3[[1]] + plts3[[2]] +
  plot_layout(nrow = 1, widths = c(1,1)) &
   theme(plot.margin = unit(c(.2,.2,.2,.2), "cm")) &
   theme(plot.background = element_rect(color  = "#fffff8", size = 2,linetype = 'dotted', fill ="#fffff8")) &
   theme(text=element_text(size=14,  family="serif"))
```



```{r}
ggsave("../Figures/qjep_ex_paths.pdf",
      ( plts1[[1]] + plts1[[2]]) /( plts3[[1]] + plts3[[3]]), 
       width = 8, height = 8)

rm(trls_easy, trls_mid, trls_hard, plts, qs, plts1, plts2, plts3)
```


# Trial level


```{r}

a %>% group_by(condition, observer, trial) %>%
  summarise(meanb = mean(b),
            prop_best = mean(selected_max), .groups = "drop") -> a_trl

ggplot(a_trl, aes(trial, meanb, group = observer)) + geom_jitter(alpha = 0.25) + 
  geom_path(alpha = 0.1) + facet_wrap(~condition) +
  geom_smooth(method = "lm", group = 1)

```


# Path Length Analysis

It looks like some of the target selections that our model fails to capture are when participants select a further away target in order to minimise overall path length. Here we will compare our model's predicted path length against the empirical paths. 

- We will provide the model/simulation with the initial target selection.

```{r, fig.height=3.5, fig.cap = "(*left*) Histogram comparing the empirical path lengths to simulations from our fitted model. Difference indicates the human path length - the simulated path length. (*right*) Histogram comparing the median path lengths per participant.", message=FALSE}
path_lengths <- readRDS("scratch/path_lengths.rda") %>%
  rename(observer = "obs",
         condition = "cond",
         human = "human_path_length",
         model = "model_path_length") %>% 
  mutate(difference  = human - model) %>%
  pivot_longer(c(human, model, difference), names_to = "type", values_to = "path_length")

ggplot(path_lengths, aes(path_length, fill = type)) + 
  geom_histogram(bins = 25, alpha = 0.5, position = "identity") +
  geom_vline(xintercept = 0, linetype = 2) + 
  theme(legend.position = "bottom")  +
  scale_x_continuous("path length")  +
  scale_x_continuous("median path length") +
  scale_fill_manual(values = col3) -> plt1

path_lengths %>% group_by(observer, type) %>%
  summarise(path_length = median(path_length), .groups = "drop") %>%
  ggplot(aes(path_length, fill = type)) + 
  geom_histogram(bins = 25, alpha = 0.5, position = "identity") +
  geom_vline(xintercept = 0, linetype = 2) + 
  theme(legend.position = "bottom") +
  scale_x_continuous("median path length") +
  scale_fill_manual(values = col3) -> plt2

plt1 + plt2 + plot_layout(guides = "collect")  &
  theme(legend.position = 'bottom', legend.title = element_blank())

# regen with legend in a different position for saving to pdf (see below)
path_lengths %>% group_by(observer, type) %>%
  summarise(path_length = median(path_length), .groups = "drop") %>%
  ggplot(aes(path_length, fill = type)) + 
  geom_histogram(bins = 25, alpha = 0.5, position = "identity") +
  geom_vline(xintercept = 0, linetype = 2) + 
  theme(legend.position = "right") +
  scale_x_continuous("median path length") +
  scale_fill_manual(values = col3) -> plt2
```

We can see that overall, the model is not making substantially longer paths through the items than our human participants. However, there is some variability in that some participants routinely select paths shorter than our model would, and others do not.

Perhaps this is related to predictability (i.e., how well the model can predict a participant's trial level behaviour)?

```{r fig.height=3, fig.cap = "(*left*) A histogram showing the proportion of trials each participant managed a shorter path through the targets than the model's prediction. (*right*) Scatter plot showing the relationship between predictability (accuracy) and whether the model over or under estimates path length. Xs mark the outlier participants discussed above."}
path_lengths %>% filter(type == "difference") %>%
  mutate(shorter = path_length < 0) %>%
  group_by(observer, condition) %>%
  summarise(prop_shorter = mean(shorter), .groups = "drop") -> path_lengths_obs

path_lengths_obs %>%
  ggplot(aes(prop_shorter, fill = condition)) + 
  geom_histogram(alpha = 0.5, breaks = seq(0, 1, 0.2), position = "identity") +
  scale_x_continuous("prop. shorter than model", breaks = seq(0, 1, 0.2)) -> plt1

path_lengths <- full_join(path_lengths_obs, fit,
                          by = c("observer", "condition"))

ggplot(path_lengths, aes(accuracy, prop_shorter, colour = condition)) + 
  geom_point(aes(shape = is_outlier)) +
  geom_smooth(data = filter(path_lengths, is_outlier == 0), 
              method = "lm", formula = y ~ x, se = FALSE) + 
  scale_shape_manual(values = c(19, 4), guide = "none") + 
  scale_y_continuous("prop. shorter than model", breaks = seq(0, 1, 0.2)) +
  coord_cartesian(ylim = c(0, 1)) -> plt2b

plt1 + plt2b + plot_layout(guides = "collect")  &
  theme(legend.position = 'bottom')
```

```{r}
ggsave("../Figures/path_length.pdf", plt2 + plt2b, width = 8, height = 4)
```

```{r}
rm(path_lengths, plt1, plt2)
```

# Initial Selection

Previous work (cite Clarke and Tatler etc) has demonstrated that the initial fixation in scene viewing can be well described with truncated Gaussian distribution. Here we explore using a similar idea to account for starting locations. 

```{r echo = TRUE}
# Reimport data again (as we removed the initial target selection earlier)
d <- read_csv("data/clarke_2020_qjep.csv", show_col_types = FALSE) %>%
  filter(found == 1) %>%
  mutate(condition = as_factor(condition))
```

```{r fig.height = 3.5, fig.cap="(*left*) Density plot for initial target selections, over all participants and all trials. (*right*) Median $x$ and $y$ coordinates for the initial target selection for each participant."}
my_theme <- theme(axis.line=element_blank(),axis.text.x=element_blank(),
          axis.text.y=element_blank(),axis.ticks=element_blank(),
          axis.title.x=element_blank(),
          axis.title.y=element_blank(),legend.position="none",
          panel.background=element_rect(color  = "#808080", size = 2,linetype = 'dotted', fill ="#808080"),
        panel.border= element_rect(colour = "black", fill=NA, size=5),
        panel.grid.major=element_blank(),
        panel.grid.minor=element_blank())

ggplot(d, aes(x, 1 - y)) + geom_hex(aes(colour = ..count..), bins = 12) +
  scale_fill_viridis_c() + scale_color_viridis_c() +
  my_theme -> plt_all


d %>% group_by(observer, condition) %>% 
  summarise(x = median(x), y = median(y),
            .groups = "drop") %>%
  ggplot(aes(x, 1 - y)) + geom_hex(bins = 12) +
  scale_fill_viridis_c()+ scale_color_viridis_c() +
  my_theme -> plt_med

plt_all + plt_med 
ggsave("../Figures/init_sel_hex_plot.pdf", width = 8, height = 4)

rm(plt_all, plt_med)
```

## Multi-level Model

We can model this as multi-level beta distribution? Yes, although it seems like people are in one of two clusters. 

```{r fig.cap = "The mean posterior plot for the truncated normal distriubtions for (*left*) $x$ and (*right*) $y$. We can see that a two component mixture model will capture most of the variance in intial target selection."}

m <- readRDS("scratch/init_sel_model.rds")

gather_draws(m, u[dim, observer]) %>% 
  ungroup() %>%
  select(-.variable) %>%
  rename(obs_b = ".value") %>%
  left_join(gather_draws(m, b[dim]), 
            by = c("dim", ".chain", ".iteration", ".draw")) %>%
  mutate(obs_b = exp(.value + obs_b),
         .variable = if_else(dim %in% c(1,2), "a", "b"),
         dim = if_else(dim %in% c(1,3), "x", "y")) %>%
  select(observer, dim, .iteration, obs_b, param = ".variable") -> a 

a %>% group_by(observer, param, dim) %>%
  summarise(value = mean(obs_b) , .groups = "drop") %>%
  pivot_wider(names_from = param, values_from = value) -> am

compute_beta_dist <- function(observer, a, b) {
  x <- seq(0.01,0.99,0.01)
  return(tibble(observer = observer, 
                x=  x,
                z = dbeta(x, a, b)))

}

create_post_plot <- function(dm) {
  
   pmap_df(filter(am,  dim == dm) %>% select(observer, a, b), 
                      compute_beta_dist) %>%
    ggplot(aes(x = x, y = z, group = observer)) + 
    geom_path(alpha = 0.25) +
    scale_x_continuous(dm) + 
    scale_y_continuous("likelihood") -> plt
  
  return(plt)
}

am %>% mutate(mu = a/(a+b)) %>%
  pivot_wider(observer, names_from = "dim", values_from = "mu") %>%
  ggplot(aes(x, y)) + geom_point(alpha = 0.5) +
  scale_x_continuous("mean x") + 
  scale_y_continuous("mean y") -> plt_mu


create_post_plot("x") + create_post_plot("y") + plt_mu
ggsave("../Figures/init_sel_mdl.pdf", width = 10, height = 3.5)
```


```{r}
# Compute weights for later
d <- read_csv("data/clarke_2020_qjep.csv", show_col_types = FALSE) %>%
  mutate(x = if_else(x < 0.01, 0.01, x),
         x = if_else(x > 0.99, 0.99, x),
         y = if_else(y < 0.01, 0.01, y),
         y = if_else(y > 0.99, 0.99, y) ) %>%
  select(-RT)

get_model_weight <- function(obs, cond, trl) {
  
  dt <- filter(d, observer == obs, condition == cond, trial == trl)
  
  mtx <- filter(am, observer == obs, dim == "x")
  mty <- filter(am, observer == obs, dim == "y")
  
  wx <- dbeta(dt$x, mtx$a, mtx$b)
  wy <- dbeta(dt$y, mty$a, mty$b)
  w <- wx * wy
  w <- w / sum(w)
  
  return(tibble(observer = obs,
                condition = cond, 
                trial = trl,
                init_weight = w[1],
                max_item = which(w == max(w))))
}

d %>% group_by(observer, condition, trial) %>%
  summarise(n = n(), .groups = "drop") %>%
  select(-n, obs = "observer", cond = "condition", trl = "trial") %>%
  pmap_df(get_model_weight) -> weights

```

## Mixture Model

It looks like we can simplify the model somewhat by using a two component mixture model instead!

```{r, fig.height = 6, fig.cap = "(*top*) Beta distributions for $x$ and $y$ coordinated"}

m <- readRDS("scratch/init_sel_model2.rds")


sample_beta <- function(c, .draw, dim, a, b) {
  x <- seq(0.01, 0.99, 0.01)
  return(tibble(c = c, 
                draw = .draw,
                dim = dim,
                x = x,
                z = dbeta(x, shape1 = a, shape2 = b)))
}

gather_draws(m, a_x[c], b_x[c], a_y[c], b_y[c], ndraws = 100) %>%
  separate(.variable, into = c("param", "dim")) %>%
  select(-.chain, -.iteration) %>%
  pivot_wider(names_from = param, values_from = ".value") %>%
  pmap_df(sample_beta) %>%
  mutate(c = as_factor(c)) -> plt_dat


plt_dat %>% filter(dim == "x") %>%
  ggplot(aes(x, z, group = interaction(c, draw), colour = c)) +
  geom_path(alpha = 0.1) + 
  facet_wrap(~dim) + 
  theme(legend.position = "none")  + 
    scale_y_continuous("likelihood") -> plt_x

plt_dat %>% filter(dim == "y") %>%
  ggplot(aes(x, z, group = interaction(c, draw), colour = c)) +
  geom_path(alpha = 0.1) + 
  facet_wrap(~dim) + 
  theme(legend.position = "none",
        axis.title.y = element_blank()) +
  scale_x_continuous("y") + 
    scale_y_continuous("likelihood") -> plt_y

gather_draws(m, lambda[observer]) %>%
  median_hdci(.value) %>%
  arrange(.value) %>%
  mutate(observer = factor(observer, levels = observer)) %>%
  ggplot(aes(observer, .value, ymin = .lower, ymax = .upper)) + 
  geom_errorbar() +
  scale_y_continuous("lambda") + 
  theme(axis.text.x=element_blank(),
        axis.ticks.x=element_blank()) -> plt_lambdas

(plt_x + plt_y) + plt_lambdas

ggsave("../Figures/init_sel2_mdl.pdf", width = 10, height = 3.5)
```



## Posterior Predictions

```{r}
gather_draws(m, a_x[c], b_x[c], a_y[c], b_y[c]) %>%
  group_by(c, .variable) %>%
  summarise(value = mean(.value), .groups = "drop") %>%
  pivot_wider(names_from = ".variable", values_from = "value") -> am

c1 <- filter(am, c==1)
c2 <- filter(am, c==2)

gather_draws(m, lambda[observer]) %>%
  group_by(observer, .variable) %>%
  summarise(value = mean(.value), .groups = "drop") %>%
  select(-.variable) -> lambdas

get_model_weight <- function(obs, cond, trl) {
  
  dt <- filter(d, observer == obs, condition == cond, trial == trl)
  
  lambda <- filter(lambdas, observer == obs)$value
  

  wx <- lambda * dbeta(dt$x, c1$a_x, c1$b_x) +
    (1-lambda) * dbeta(dt$x, c2$a_x, c2$b_x) 
  
  wy <- lambda * dbeta(dt$y, c1$a_y, c1$b_y) +
    (1-lambda) * dbeta(dt$y, c2$a_y, c2$b_y) 
  
  w <- wx * wy
  w <- w / sum(w)
  
  return(tibble(observer = obs,
                condition = cond, 
                trial = trl,
                init_weight = w[1],
                max_item = which(w == max(w))))
}


d %>% group_by(observer, condition, trial) %>%
  summarise(n = n(), .groups = "drop") %>%
  select(-n, obs = "observer", cond = "condition", trl = "trial") %>%
  pmap_df(get_model_weight) -> weights2


bind_rows(weights %>% mutate(method = "multi-level"), 
          weights2 %>% mutate(method = "mixture model")) -> dw


ggplot(dw, aes(init_weight, fill = method)) + 
  geom_histogram(binwidth = 1/80, position = "identity", alpha = 0.5) +
  geom_vline(xintercept = 1/40, linetype = 2) +
  scale_fill_manual(values = col2)-> plt1

dw %>% group_by(observer, method) %>%
  summarise(acc = mean(max_item == 1), .groups = "drop") %>%
  ggplot(aes(y = acc, fill = method)) + geom_boxplot(alpha = 0.5) + 
  geom_hline(yintercept = 1/40, linetype = 2) + 
  scale_y_continuous("Prop. trials first sel. correct") +
  theme(legend.position = "none") +  
  scale_fill_manual(values = col2) -> plt2

weights2 %>% group_by(observer) %>%
  summarise(accuracy = mean(max_item == 1), .groups = "drop") %>%
  full_join(lambdas, by = "observer") %>%
  rename(lambda = "value") %>%
  ggplot(aes(lambda, accuracy)) + 
  geom_point() +
  geom_smooth(method = "lm", formula = y ~ x, colour = "black") -> plt3

plt1 / (plt2 + plt3)  + plot_layout(guides = "collect")

ggsave("../Figures/qjep_init_sel_preds.pdf", 
       plt1 / (plt2 + plt3)  + plot_layout(guides = "collect"),
       width = 10, height = 6)

dw %>% group_by(method, observer) %>%
  summarise(score = median(init_weight,),
            accuracy = mean(max_item==1),
            .groups = "drop_last") %>%
  summarise(mean_score = mean(score),
            mean_accuracy = mean(accuracy),
            .groups = "drop") %>%
  knitr::kable()

```

